{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T03:56:58.431790Z",
     "start_time": "2021-12-12T03:56:56.643473Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T03:56:58.441424Z",
     "start_time": "2021-12-12T03:56:58.433941Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "HIDDEN_SIZE = 256\n",
    "LOAD_MODEL = False\n",
    "USE_TRANSFER_LEARNING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T03:56:58.450548Z",
     "start_time": "2021-12-12T03:56:58.444173Z"
    }
   },
   "outputs": [],
   "source": [
    "class Graph2VecEmbeddingsDataset(Dataset):\n",
    "    \"\"\"Graph2Vec Embeddings dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, embeddings, labels=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings (pd.dataframe): Pandas Dataframe with the graph2vec embeddings\n",
    "            labels : Labels indicating intelligence for the respective individual\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        sample = torch.tensor(self.embeddings.iloc[idx]).float()\n",
    "\n",
    "        if self.labels is not None:\n",
    "            return sample, torch.tensor(self.labels.iloc[idx]).float()\n",
    "        else:\n",
    "            return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_y = pd.read_csv(\"intelligence_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subjectkey</th>\n",
       "      <th>nihtbx_totalcomp_uncorrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NDARINV003RTV85</td>\n",
       "      <td>-0.790265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDARINV007W6H7B</td>\n",
       "      <td>-0.571433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NDARINV00BD7VDC</td>\n",
       "      <td>0.632147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NDARINV00LJVZK2</td>\n",
       "      <td>0.194482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NDARINV00NPMHND</td>\n",
       "      <td>-0.680849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6907</th>\n",
       "      <td>NDARINVZZL0VA2F</td>\n",
       "      <td>-0.680849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6908</th>\n",
       "      <td>NDARINVZZLZCKAY</td>\n",
       "      <td>-0.462016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6909</th>\n",
       "      <td>NDARINVZZPKBDAC</td>\n",
       "      <td>-0.680849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6910</th>\n",
       "      <td>NDARINVZZZ2ALR6</td>\n",
       "      <td>-0.243184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6911</th>\n",
       "      <td>NDARINVZZZNB0XC</td>\n",
       "      <td>0.632147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6912 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           subjectkey  nihtbx_totalcomp_uncorrected\n",
       "0     NDARINV003RTV85                     -0.790265\n",
       "1     NDARINV007W6H7B                     -0.571433\n",
       "2     NDARINV00BD7VDC                      0.632147\n",
       "3     NDARINV00LJVZK2                      0.194482\n",
       "4     NDARINV00NPMHND                     -0.680849\n",
       "...               ...                           ...\n",
       "6907  NDARINVZZL0VA2F                     -0.680849\n",
       "6908  NDARINVZZLZCKAY                     -0.462016\n",
       "6909  NDARINVZZPKBDAC                     -0.680849\n",
       "6910  NDARINVZZZ2ALR6                     -0.243184\n",
       "6911  NDARINVZZZNB0XC                      0.632147\n",
       "\n",
       "[6912 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T03:56:58.543231Z",
     "start_time": "2021-12-12T03:56:58.452256Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read data, make fake y and drop the \"type\" column\n",
    "\n",
    "#data = pd.read_csv(\"data/nci1.csv\")\n",
    "#data['y'] = np.random.normal(100,30, size=len(data))\n",
    "#data = data.drop(\"type\", axis=1)\n",
    "\n",
    "data = pd.read_csv(\"features/func_embedding.csv\")\n",
    "#cols_to_norm = data.columns#.drop(\"y\")\n",
    "#data[cols_to_norm]=(data[cols_to_norm]-data[cols_to_norm].mean())/data[cols_to_norm].std()\n",
    "#data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjid=data['type'].values[9].split('/')[-1].split('_')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NDARINV040B4TRC'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['type'].values[9][-15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/Stella/MLVU_multimodality/graph2vec/structural_graph_for_graph2vec_1212/graph2vec_structural_graph_NDARINV003RTV85'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['type'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NDARINV040B4TRC'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NDARINV003RTV85'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_y['subjectkey'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for i in data['type']:\n",
    "    subjid = i.split('/')[-1].split('_')[-1]\n",
    "    for j in range(len(raw_y)):\n",
    "        if subjid == raw_y['subjectkey'][j]:\n",
    "            y.append(raw_y['nihtbx_totalcomp_uncorrected'][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2085"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>x_0</th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>x_6</th>\n",
       "      <th>x_7</th>\n",
       "      <th>x_8</th>\n",
       "      <th>...</th>\n",
       "      <th>x_118</th>\n",
       "      <th>x_119</th>\n",
       "      <th>x_120</th>\n",
       "      <th>x_121</th>\n",
       "      <th>x_122</th>\n",
       "      <th>x_123</th>\n",
       "      <th>x_124</th>\n",
       "      <th>x_125</th>\n",
       "      <th>x_126</th>\n",
       "      <th>x_127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/ubuntu/Stella/MLVU_multimodality/graph2v...</td>\n",
       "      <td>-0.060564</td>\n",
       "      <td>-0.090097</td>\n",
       "      <td>0.017879</td>\n",
       "      <td>-0.233405</td>\n",
       "      <td>-0.031025</td>\n",
       "      <td>-0.161997</td>\n",
       "      <td>-0.110498</td>\n",
       "      <td>-0.106946</td>\n",
       "      <td>0.161249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154834</td>\n",
       "      <td>-0.357470</td>\n",
       "      <td>-0.041721</td>\n",
       "      <td>0.027009</td>\n",
       "      <td>0.074247</td>\n",
       "      <td>0.188864</td>\n",
       "      <td>-0.069622</td>\n",
       "      <td>-0.065493</td>\n",
       "      <td>-0.019565</td>\n",
       "      <td>-0.100768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/ubuntu/Stella/MLVU_multimodality/graph2v...</td>\n",
       "      <td>-0.058420</td>\n",
       "      <td>-0.086559</td>\n",
       "      <td>0.019345</td>\n",
       "      <td>-0.226949</td>\n",
       "      <td>-0.031319</td>\n",
       "      <td>-0.155380</td>\n",
       "      <td>-0.109569</td>\n",
       "      <td>-0.100978</td>\n",
       "      <td>0.149402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149971</td>\n",
       "      <td>-0.341140</td>\n",
       "      <td>-0.044229</td>\n",
       "      <td>0.026961</td>\n",
       "      <td>0.069826</td>\n",
       "      <td>0.183943</td>\n",
       "      <td>-0.067535</td>\n",
       "      <td>-0.063860</td>\n",
       "      <td>-0.019622</td>\n",
       "      <td>-0.093313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/ubuntu/Stella/MLVU_multimodality/graph2v...</td>\n",
       "      <td>-0.066414</td>\n",
       "      <td>-0.084501</td>\n",
       "      <td>0.019682</td>\n",
       "      <td>-0.234007</td>\n",
       "      <td>-0.033996</td>\n",
       "      <td>-0.160188</td>\n",
       "      <td>-0.116341</td>\n",
       "      <td>-0.105766</td>\n",
       "      <td>0.160227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154652</td>\n",
       "      <td>-0.353492</td>\n",
       "      <td>-0.039726</td>\n",
       "      <td>0.028335</td>\n",
       "      <td>0.074511</td>\n",
       "      <td>0.186440</td>\n",
       "      <td>-0.072687</td>\n",
       "      <td>-0.069850</td>\n",
       "      <td>-0.023465</td>\n",
       "      <td>-0.104869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/ubuntu/Stella/MLVU_multimodality/graph2v...</td>\n",
       "      <td>-0.061569</td>\n",
       "      <td>-0.079573</td>\n",
       "      <td>0.018303</td>\n",
       "      <td>-0.221120</td>\n",
       "      <td>-0.028189</td>\n",
       "      <td>-0.150833</td>\n",
       "      <td>-0.103980</td>\n",
       "      <td>-0.096945</td>\n",
       "      <td>0.152231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146012</td>\n",
       "      <td>-0.330881</td>\n",
       "      <td>-0.039162</td>\n",
       "      <td>0.021560</td>\n",
       "      <td>0.068880</td>\n",
       "      <td>0.172749</td>\n",
       "      <td>-0.061961</td>\n",
       "      <td>-0.060701</td>\n",
       "      <td>-0.015221</td>\n",
       "      <td>-0.097385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/ubuntu/Stella/MLVU_multimodality/graph2v...</td>\n",
       "      <td>-0.063210</td>\n",
       "      <td>-0.087080</td>\n",
       "      <td>0.025141</td>\n",
       "      <td>-0.234608</td>\n",
       "      <td>-0.028360</td>\n",
       "      <td>-0.162248</td>\n",
       "      <td>-0.116670</td>\n",
       "      <td>-0.104763</td>\n",
       "      <td>0.154640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151280</td>\n",
       "      <td>-0.348356</td>\n",
       "      <td>-0.039079</td>\n",
       "      <td>0.024286</td>\n",
       "      <td>0.072855</td>\n",
       "      <td>0.182617</td>\n",
       "      <td>-0.066336</td>\n",
       "      <td>-0.065799</td>\n",
       "      <td>-0.023753</td>\n",
       "      <td>-0.103925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080</th>\n",
       "      <td>/home/ubuntu/Stella/MLVU_multimodality/graph2v...</td>\n",
       "      <td>-0.052150</td>\n",
       "      <td>-0.077419</td>\n",
       "      <td>0.014720</td>\n",
       "      <td>-0.209189</td>\n",
       "      <td>-0.026127</td>\n",
       "      <td>-0.145545</td>\n",
       "      <td>-0.099978</td>\n",
       "      <td>-0.094635</td>\n",
       "      <td>0.141908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137550</td>\n",
       "      <td>-0.313037</td>\n",
       "      <td>-0.037452</td>\n",
       "      <td>0.020052</td>\n",
       "      <td>0.067365</td>\n",
       "      <td>0.168734</td>\n",
       "      <td>-0.060060</td>\n",
       "      <td>-0.064484</td>\n",
       "      <td>-0.016375</td>\n",
       "      <td>-0.086581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2081</th>\n",
       "      <td>/home/ubuntu/Stella/MLVU_multimodality/graph2v...</td>\n",
       "      <td>-0.056559</td>\n",
       "      <td>-0.080355</td>\n",
       "      <td>0.022327</td>\n",
       "      <td>-0.229628</td>\n",
       "      <td>-0.029295</td>\n",
       "      <td>-0.160573</td>\n",
       "      <td>-0.113333</td>\n",
       "      <td>-0.101679</td>\n",
       "      <td>0.155368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149303</td>\n",
       "      <td>-0.339774</td>\n",
       "      <td>-0.042790</td>\n",
       "      <td>0.025548</td>\n",
       "      <td>0.072326</td>\n",
       "      <td>0.176757</td>\n",
       "      <td>-0.066028</td>\n",
       "      <td>-0.068545</td>\n",
       "      <td>-0.019505</td>\n",
       "      <td>-0.094861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2082</th>\n",
       "      <td>/home/ubuntu/Stella/MLVU_multimodality/graph2v...</td>\n",
       "      <td>-0.059904</td>\n",
       "      <td>-0.084092</td>\n",
       "      <td>0.022182</td>\n",
       "      <td>-0.218934</td>\n",
       "      <td>-0.033377</td>\n",
       "      <td>-0.157618</td>\n",
       "      <td>-0.110358</td>\n",
       "      <td>-0.098928</td>\n",
       "      <td>0.148119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149383</td>\n",
       "      <td>-0.332917</td>\n",
       "      <td>-0.036531</td>\n",
       "      <td>0.026738</td>\n",
       "      <td>0.067816</td>\n",
       "      <td>0.179977</td>\n",
       "      <td>-0.069771</td>\n",
       "      <td>-0.069681</td>\n",
       "      <td>-0.022913</td>\n",
       "      <td>-0.093519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>/home/ubuntu/Stella/MLVU_multimodality/graph2v...</td>\n",
       "      <td>-0.062419</td>\n",
       "      <td>-0.086961</td>\n",
       "      <td>0.018969</td>\n",
       "      <td>-0.241844</td>\n",
       "      <td>-0.030937</td>\n",
       "      <td>-0.168730</td>\n",
       "      <td>-0.120848</td>\n",
       "      <td>-0.102684</td>\n",
       "      <td>0.162021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152759</td>\n",
       "      <td>-0.365104</td>\n",
       "      <td>-0.042149</td>\n",
       "      <td>0.024744</td>\n",
       "      <td>0.076086</td>\n",
       "      <td>0.194581</td>\n",
       "      <td>-0.067730</td>\n",
       "      <td>-0.070011</td>\n",
       "      <td>-0.015568</td>\n",
       "      <td>-0.100741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>/home/ubuntu/Stella/MLVU_multimodality/graph2v...</td>\n",
       "      <td>-0.061167</td>\n",
       "      <td>-0.087081</td>\n",
       "      <td>0.017005</td>\n",
       "      <td>-0.221845</td>\n",
       "      <td>-0.031672</td>\n",
       "      <td>-0.159279</td>\n",
       "      <td>-0.111235</td>\n",
       "      <td>-0.100327</td>\n",
       "      <td>0.156237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149118</td>\n",
       "      <td>-0.338900</td>\n",
       "      <td>-0.043286</td>\n",
       "      <td>0.025112</td>\n",
       "      <td>0.065961</td>\n",
       "      <td>0.175440</td>\n",
       "      <td>-0.064622</td>\n",
       "      <td>-0.065419</td>\n",
       "      <td>-0.020433</td>\n",
       "      <td>-0.091969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2085 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   type       x_0       x_1  \\\n",
       "0     /home/ubuntu/Stella/MLVU_multimodality/graph2v... -0.060564 -0.090097   \n",
       "1     /home/ubuntu/Stella/MLVU_multimodality/graph2v... -0.058420 -0.086559   \n",
       "2     /home/ubuntu/Stella/MLVU_multimodality/graph2v... -0.066414 -0.084501   \n",
       "3     /home/ubuntu/Stella/MLVU_multimodality/graph2v... -0.061569 -0.079573   \n",
       "4     /home/ubuntu/Stella/MLVU_multimodality/graph2v... -0.063210 -0.087080   \n",
       "...                                                 ...       ...       ...   \n",
       "2080  /home/ubuntu/Stella/MLVU_multimodality/graph2v... -0.052150 -0.077419   \n",
       "2081  /home/ubuntu/Stella/MLVU_multimodality/graph2v... -0.056559 -0.080355   \n",
       "2082  /home/ubuntu/Stella/MLVU_multimodality/graph2v... -0.059904 -0.084092   \n",
       "2083  /home/ubuntu/Stella/MLVU_multimodality/graph2v... -0.062419 -0.086961   \n",
       "2084  /home/ubuntu/Stella/MLVU_multimodality/graph2v... -0.061167 -0.087081   \n",
       "\n",
       "           x_2       x_3       x_4       x_5       x_6       x_7       x_8  \\\n",
       "0     0.017879 -0.233405 -0.031025 -0.161997 -0.110498 -0.106946  0.161249   \n",
       "1     0.019345 -0.226949 -0.031319 -0.155380 -0.109569 -0.100978  0.149402   \n",
       "2     0.019682 -0.234007 -0.033996 -0.160188 -0.116341 -0.105766  0.160227   \n",
       "3     0.018303 -0.221120 -0.028189 -0.150833 -0.103980 -0.096945  0.152231   \n",
       "4     0.025141 -0.234608 -0.028360 -0.162248 -0.116670 -0.104763  0.154640   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2080  0.014720 -0.209189 -0.026127 -0.145545 -0.099978 -0.094635  0.141908   \n",
       "2081  0.022327 -0.229628 -0.029295 -0.160573 -0.113333 -0.101679  0.155368   \n",
       "2082  0.022182 -0.218934 -0.033377 -0.157618 -0.110358 -0.098928  0.148119   \n",
       "2083  0.018969 -0.241844 -0.030937 -0.168730 -0.120848 -0.102684  0.162021   \n",
       "2084  0.017005 -0.221845 -0.031672 -0.159279 -0.111235 -0.100327  0.156237   \n",
       "\n",
       "      ...     x_118     x_119     x_120     x_121     x_122     x_123  \\\n",
       "0     ...  0.154834 -0.357470 -0.041721  0.027009  0.074247  0.188864   \n",
       "1     ...  0.149971 -0.341140 -0.044229  0.026961  0.069826  0.183943   \n",
       "2     ...  0.154652 -0.353492 -0.039726  0.028335  0.074511  0.186440   \n",
       "3     ...  0.146012 -0.330881 -0.039162  0.021560  0.068880  0.172749   \n",
       "4     ...  0.151280 -0.348356 -0.039079  0.024286  0.072855  0.182617   \n",
       "...   ...       ...       ...       ...       ...       ...       ...   \n",
       "2080  ...  0.137550 -0.313037 -0.037452  0.020052  0.067365  0.168734   \n",
       "2081  ...  0.149303 -0.339774 -0.042790  0.025548  0.072326  0.176757   \n",
       "2082  ...  0.149383 -0.332917 -0.036531  0.026738  0.067816  0.179977   \n",
       "2083  ...  0.152759 -0.365104 -0.042149  0.024744  0.076086  0.194581   \n",
       "2084  ...  0.149118 -0.338900 -0.043286  0.025112  0.065961  0.175440   \n",
       "\n",
       "         x_124     x_125     x_126     x_127  \n",
       "0    -0.069622 -0.065493 -0.019565 -0.100768  \n",
       "1    -0.067535 -0.063860 -0.019622 -0.093313  \n",
       "2    -0.072687 -0.069850 -0.023465 -0.104869  \n",
       "3    -0.061961 -0.060701 -0.015221 -0.097385  \n",
       "4    -0.066336 -0.065799 -0.023753 -0.103925  \n",
       "...        ...       ...       ...       ...  \n",
       "2080 -0.060060 -0.064484 -0.016375 -0.086581  \n",
       "2081 -0.066028 -0.068545 -0.019505 -0.094861  \n",
       "2082 -0.069771 -0.069681 -0.022913 -0.093519  \n",
       "2083 -0.067730 -0.070011 -0.015568 -0.100741  \n",
       "2084 -0.064622 -0.065419 -0.020433 -0.091969  \n",
       "\n",
       "[2085 rows x 129 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['y'] = y\n",
    "data = data.drop(\"type\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_0</th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>x_6</th>\n",
       "      <th>x_7</th>\n",
       "      <th>x_8</th>\n",
       "      <th>x_9</th>\n",
       "      <th>...</th>\n",
       "      <th>x_119</th>\n",
       "      <th>x_120</th>\n",
       "      <th>x_121</th>\n",
       "      <th>x_122</th>\n",
       "      <th>x_123</th>\n",
       "      <th>x_124</th>\n",
       "      <th>x_125</th>\n",
       "      <th>x_126</th>\n",
       "      <th>x_127</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.004466</td>\n",
       "      <td>0.003101</td>\n",
       "      <td>-0.052307</td>\n",
       "      <td>0.008698</td>\n",
       "      <td>0.030163</td>\n",
       "      <td>-0.007314</td>\n",
       "      <td>-0.044728</td>\n",
       "      <td>0.034471</td>\n",
       "      <td>0.006349</td>\n",
       "      <td>0.044104</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045145</td>\n",
       "      <td>-0.035233</td>\n",
       "      <td>0.033590</td>\n",
       "      <td>-0.003432</td>\n",
       "      <td>0.033996</td>\n",
       "      <td>0.029607</td>\n",
       "      <td>0.008686</td>\n",
       "      <td>-0.028766</td>\n",
       "      <td>0.067319</td>\n",
       "      <td>-0.790265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.003428</td>\n",
       "      <td>-0.028072</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>0.018698</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>-0.023380</td>\n",
       "      <td>0.012970</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>0.019547</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021222</td>\n",
       "      <td>-0.019916</td>\n",
       "      <td>0.022018</td>\n",
       "      <td>0.003037</td>\n",
       "      <td>0.020805</td>\n",
       "      <td>0.014551</td>\n",
       "      <td>0.007052</td>\n",
       "      <td>-0.009735</td>\n",
       "      <td>0.033356</td>\n",
       "      <td>-0.680849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.008482</td>\n",
       "      <td>0.035694</td>\n",
       "      <td>-0.149806</td>\n",
       "      <td>0.076943</td>\n",
       "      <td>0.148706</td>\n",
       "      <td>-0.061347</td>\n",
       "      <td>-0.188420</td>\n",
       "      <td>0.181947</td>\n",
       "      <td>0.057129</td>\n",
       "      <td>0.183347</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102514</td>\n",
       "      <td>-0.109775</td>\n",
       "      <td>0.158490</td>\n",
       "      <td>-0.032627</td>\n",
       "      <td>0.073411</td>\n",
       "      <td>0.106722</td>\n",
       "      <td>0.041687</td>\n",
       "      <td>-0.089557</td>\n",
       "      <td>0.197947</td>\n",
       "      <td>0.632147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.026975</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>-0.263654</td>\n",
       "      <td>0.040010</td>\n",
       "      <td>0.174837</td>\n",
       "      <td>-0.030992</td>\n",
       "      <td>-0.229716</td>\n",
       "      <td>0.178273</td>\n",
       "      <td>0.020650</td>\n",
       "      <td>0.208909</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.231228</td>\n",
       "      <td>-0.167932</td>\n",
       "      <td>0.187091</td>\n",
       "      <td>-0.006038</td>\n",
       "      <td>0.168102</td>\n",
       "      <td>0.160195</td>\n",
       "      <td>0.068691</td>\n",
       "      <td>-0.139464</td>\n",
       "      <td>0.327302</td>\n",
       "      <td>-0.133767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.001829</td>\n",
       "      <td>-0.001446</td>\n",
       "      <td>0.026109</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>-0.014490</td>\n",
       "      <td>0.003630</td>\n",
       "      <td>0.020709</td>\n",
       "      <td>-0.016144</td>\n",
       "      <td>-0.004370</td>\n",
       "      <td>-0.019283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018195</td>\n",
       "      <td>0.016880</td>\n",
       "      <td>-0.020034</td>\n",
       "      <td>0.002513</td>\n",
       "      <td>-0.019045</td>\n",
       "      <td>-0.013067</td>\n",
       "      <td>-0.003425</td>\n",
       "      <td>0.016418</td>\n",
       "      <td>-0.028255</td>\n",
       "      <td>-0.243184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080</th>\n",
       "      <td>-0.005094</td>\n",
       "      <td>-0.004634</td>\n",
       "      <td>0.021784</td>\n",
       "      <td>-0.001594</td>\n",
       "      <td>-0.015345</td>\n",
       "      <td>0.005997</td>\n",
       "      <td>0.014523</td>\n",
       "      <td>-0.015140</td>\n",
       "      <td>-0.003535</td>\n",
       "      <td>-0.016288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016145</td>\n",
       "      <td>0.010044</td>\n",
       "      <td>-0.016801</td>\n",
       "      <td>-0.002833</td>\n",
       "      <td>-0.017898</td>\n",
       "      <td>-0.013662</td>\n",
       "      <td>-0.002859</td>\n",
       "      <td>0.011401</td>\n",
       "      <td>-0.028907</td>\n",
       "      <td>0.194482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2081</th>\n",
       "      <td>0.003605</td>\n",
       "      <td>0.007045</td>\n",
       "      <td>-0.040078</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>0.022491</td>\n",
       "      <td>-0.004558</td>\n",
       "      <td>-0.033821</td>\n",
       "      <td>0.027857</td>\n",
       "      <td>0.007009</td>\n",
       "      <td>0.030754</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035346</td>\n",
       "      <td>-0.021141</td>\n",
       "      <td>0.032329</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>0.024632</td>\n",
       "      <td>0.020912</td>\n",
       "      <td>0.010401</td>\n",
       "      <td>-0.017409</td>\n",
       "      <td>0.050803</td>\n",
       "      <td>1.179229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2082</th>\n",
       "      <td>0.047037</td>\n",
       "      <td>0.024805</td>\n",
       "      <td>-0.269646</td>\n",
       "      <td>0.018161</td>\n",
       "      <td>0.177993</td>\n",
       "      <td>-0.014456</td>\n",
       "      <td>-0.228219</td>\n",
       "      <td>0.188587</td>\n",
       "      <td>0.027699</td>\n",
       "      <td>0.224862</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.237530</td>\n",
       "      <td>-0.150155</td>\n",
       "      <td>0.182934</td>\n",
       "      <td>-0.006497</td>\n",
       "      <td>0.166452</td>\n",
       "      <td>0.146688</td>\n",
       "      <td>0.059213</td>\n",
       "      <td>-0.125576</td>\n",
       "      <td>0.341123</td>\n",
       "      <td>-0.462016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>0.027972</td>\n",
       "      <td>0.017276</td>\n",
       "      <td>-0.196981</td>\n",
       "      <td>0.018430</td>\n",
       "      <td>0.132826</td>\n",
       "      <td>-0.020505</td>\n",
       "      <td>-0.176739</td>\n",
       "      <td>0.131125</td>\n",
       "      <td>0.022697</td>\n",
       "      <td>0.162833</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.166204</td>\n",
       "      <td>-0.126386</td>\n",
       "      <td>0.149962</td>\n",
       "      <td>-0.006443</td>\n",
       "      <td>0.134264</td>\n",
       "      <td>0.123120</td>\n",
       "      <td>0.053563</td>\n",
       "      <td>-0.104671</td>\n",
       "      <td>0.250376</td>\n",
       "      <td>-0.024351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>0.031521</td>\n",
       "      <td>0.016027</td>\n",
       "      <td>-0.207092</td>\n",
       "      <td>0.025289</td>\n",
       "      <td>0.133436</td>\n",
       "      <td>-0.022011</td>\n",
       "      <td>-0.176841</td>\n",
       "      <td>0.137825</td>\n",
       "      <td>0.025724</td>\n",
       "      <td>0.163050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.180038</td>\n",
       "      <td>-0.134196</td>\n",
       "      <td>0.146822</td>\n",
       "      <td>-0.007504</td>\n",
       "      <td>0.139472</td>\n",
       "      <td>0.127322</td>\n",
       "      <td>0.045819</td>\n",
       "      <td>-0.116230</td>\n",
       "      <td>0.263145</td>\n",
       "      <td>0.632147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2085 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x_0       x_1       x_2       x_3       x_4       x_5       x_6  \\\n",
       "0     0.004466  0.003101 -0.052307  0.008698  0.030163 -0.007314 -0.044728   \n",
       "1     0.000091  0.003428 -0.028072  0.002163  0.018698  0.000912 -0.023380   \n",
       "2    -0.008482  0.035694 -0.149806  0.076943  0.148706 -0.061347 -0.188420   \n",
       "3     0.026975  0.020250 -0.263654  0.040010  0.174837 -0.030992 -0.229716   \n",
       "4    -0.001829 -0.001446  0.026109  0.000369 -0.014490  0.003630  0.020709   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2080 -0.005094 -0.004634  0.021784 -0.001594 -0.015345  0.005997  0.014523   \n",
       "2081  0.003605  0.007045 -0.040078  0.000841  0.022491 -0.004558 -0.033821   \n",
       "2082  0.047037  0.024805 -0.269646  0.018161  0.177993 -0.014456 -0.228219   \n",
       "2083  0.027972  0.017276 -0.196981  0.018430  0.132826 -0.020505 -0.176739   \n",
       "2084  0.031521  0.016027 -0.207092  0.025289  0.133436 -0.022011 -0.176841   \n",
       "\n",
       "           x_7       x_8       x_9  ...     x_119     x_120     x_121  \\\n",
       "0     0.034471  0.006349  0.044104  ... -0.045145 -0.035233  0.033590   \n",
       "1     0.012970  0.001927  0.019547  ... -0.021222 -0.019916  0.022018   \n",
       "2     0.181947  0.057129  0.183347  ... -0.102514 -0.109775  0.158490   \n",
       "3     0.178273  0.020650  0.208909  ... -0.231228 -0.167932  0.187091   \n",
       "4    -0.016144 -0.004370 -0.019283  ...  0.018195  0.016880 -0.020034   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2080 -0.015140 -0.003535 -0.016288  ...  0.016145  0.010044 -0.016801   \n",
       "2081  0.027857  0.007009  0.030754  ... -0.035346 -0.021141  0.032329   \n",
       "2082  0.188587  0.027699  0.224862  ... -0.237530 -0.150155  0.182934   \n",
       "2083  0.131125  0.022697  0.162833  ... -0.166204 -0.126386  0.149962   \n",
       "2084  0.137825  0.025724  0.163050  ... -0.180038 -0.134196  0.146822   \n",
       "\n",
       "         x_122     x_123     x_124     x_125     x_126     x_127         y  \n",
       "0    -0.003432  0.033996  0.029607  0.008686 -0.028766  0.067319 -0.790265  \n",
       "1     0.003037  0.020805  0.014551  0.007052 -0.009735  0.033356 -0.680849  \n",
       "2    -0.032627  0.073411  0.106722  0.041687 -0.089557  0.197947  0.632147  \n",
       "3    -0.006038  0.168102  0.160195  0.068691 -0.139464  0.327302 -0.133767  \n",
       "4     0.002513 -0.019045 -0.013067 -0.003425  0.016418 -0.028255 -0.243184  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "2080 -0.002833 -0.017898 -0.013662 -0.002859  0.011401 -0.028907  0.194482  \n",
       "2081  0.001603  0.024632  0.020912  0.010401 -0.017409  0.050803  1.179229  \n",
       "2082 -0.006497  0.166452  0.146688  0.059213 -0.125576  0.341123 -0.462016  \n",
       "2083 -0.006443  0.134264  0.123120  0.053563 -0.104671  0.250376 -0.024351  \n",
       "2084 -0.007504  0.139472  0.127322  0.045819 -0.116230  0.263145  0.632147  \n",
       "\n",
       "[2085 rows x 129 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T03:56:58.555167Z",
     "start_time": "2021-12-12T03:56:58.544714Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = np.split(data.sample(frac=1, random_state=42), [int(.9*len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T03:56:58.565155Z",
     "start_time": "2021-12-12T03:56:58.557406Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = \\\n",
    "    train_test_split(train_df.iloc[:, 1:], train_df['y'], test_size=1/9, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T03:56:58.575067Z",
     "start_time": "2021-12-12T03:56:58.569068Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test = test_df[\"y\"]\n",
    "X_test = test_df.drop(\"y\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T03:56:58.583376Z",
     "start_time": "2021-12-12T03:56:58.578275Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = Graph2VecEmbeddingsDataset(embeddings=X_train, labels=y_train)\n",
    "valid_dataset = Graph2VecEmbeddingsDataset(embeddings=X_valid, labels=y_valid)\n",
    "test_dataset = Graph2VecEmbeddingsDataset(embeddings=X_test, labels=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T03:56:58.598531Z",
     "start_time": "2021-12-12T03:56:58.585628Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 6.0789e-02, -2.3346e-01,  4.2744e-02,  1.4962e-01,  2.0078e-02,\n",
       "         -1.7247e-01,  1.7276e-01,  1.5864e-02,  1.3666e-01,  5.4920e-02,\n",
       "         -5.0280e-02, -8.3688e-02,  2.9710e-01,  4.1241e-02,  9.1209e-03,\n",
       "         -1.0858e-01, -1.1591e-01, -3.5099e-03, -8.2184e-02,  4.4909e-02,\n",
       "         -1.3621e-02,  2.0660e-02,  4.4805e-02, -5.1467e-02,  1.0313e-01,\n",
       "          5.5481e-02,  1.8245e-01, -1.8429e-01,  1.2391e-02,  2.5342e-01,\n",
       "         -1.0511e-02, -1.4858e-01,  1.5569e-01,  1.0291e-02, -6.8171e-02,\n",
       "          6.1983e-02, -7.4956e-02,  5.4478e-02,  9.3476e-02, -1.7749e-02,\n",
       "          4.4634e-02, -2.3824e-02, -1.0420e-01,  1.0443e-01,  2.4010e-02,\n",
       "          2.5109e-02,  1.4724e-02, -6.0827e-02,  1.4823e-01, -1.1879e-01,\n",
       "         -3.8111e-02, -1.4845e-01,  4.7469e-02,  2.4806e-01, -1.9702e-01,\n",
       "         -1.4839e-01, -3.5616e-02, -1.8818e-01, -8.5802e-02, -5.2573e-02,\n",
       "         -4.3351e-02, -5.9268e-02,  7.1251e-02, -4.2642e-02,  1.1837e-01,\n",
       "          2.7017e-02, -5.0049e-02, -7.1928e-02,  7.8885e-02, -5.2410e-03,\n",
       "          3.4438e-02,  4.0243e-02,  6.6433e-02, -2.5339e-01,  7.8067e-02,\n",
       "         -1.5148e-02,  6.8657e-02, -3.7408e-02,  9.4100e-02, -2.8020e-03,\n",
       "         -1.2439e-01, -1.2846e-01, -9.7653e-02,  9.9034e-02,  1.8488e-01,\n",
       "         -9.3288e-02,  8.2950e-02, -2.8726e-02,  1.3913e-02,  2.1736e-01,\n",
       "          5.4487e-02,  1.8961e-01, -8.5250e-02,  6.2293e-02,  1.5522e-01,\n",
       "          1.0023e-01, -2.1651e-01,  1.4394e-01,  1.0464e-01,  5.0560e-02,\n",
       "         -3.6077e-02, -2.4139e-02,  2.1488e-04, -3.2406e-01, -2.1567e-01,\n",
       "         -1.0292e-01,  1.2648e-01,  7.4258e-02, -4.7615e-02,  2.9536e-02,\n",
       "          1.5876e-01,  1.4553e-01,  7.3387e-02,  2.6739e-02, -1.0006e-01,\n",
       "         -1.0447e-01,  5.7914e-03, -2.1705e-01, -1.8225e-01, -9.2359e-02,\n",
       "          1.5960e-01,  2.3236e-02,  1.2877e-01,  9.2654e-02,  5.2892e-02,\n",
       "         -1.4087e-01,  2.9202e-01, -8.9968e-01]),\n",
       " tensor(-0.8997))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T03:56:58.605780Z",
     "start_time": "2021-12-12T03:56:58.600486Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T03:56:58.644302Z",
     "start_time": "2021-12-12T03:56:58.609040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings shape on PyTroch :  torch.Size([32, 128])\n",
      "labels shape on PyTroch :  torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "embeddings, labels = dataiter.next()\n",
    "\n",
    "print('embeddings shape on PyTroch : ', embeddings.size())\n",
    "print('labels shape on PyTroch : ', labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T03:56:58.652697Z",
     "start_time": "2021-12-12T03:56:58.646160Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0.01)\n",
    "        \n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.layers.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T03:56:58.668977Z",
     "start_time": "2021-12-12T03:56:58.660281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.4, inplace=False)\n",
      "    (8): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (9): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.3, inplace=False)\n",
      "    (12): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MLP(embeddings.size()[1], 256)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T03:56:58.679747Z",
     "start_time": "2021-12-12T03:56:58.671957Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n",
    "                                        lr_lambda=lambda epoch: 0.95 ** epoch,\n",
    "                                        last_epoch=-1,\n",
    "                                        verbose=False)\n",
    "loss_fn = nn.MSELoss() #will calculate RMSE next line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T04:32:41.560428Z",
     "start_time": "2021-12-12T03:56:58.683228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1, train loss : 0.8455, valid loss : 0.3299\n",
      "epoch : 2, train loss : 0.6892, valid loss : 0.1758\n",
      "epoch : 3, train loss : 0.5874, valid loss : 0.1519\n",
      "epoch : 4, train loss : 0.5114, valid loss : 0.1956\n",
      "epoch : 5, train loss : 0.4394, valid loss : 0.2356\n",
      "epoch : 6, train loss : 0.4032, valid loss : 0.2233\n",
      "epoch : 7, train loss : 0.3434, valid loss : 0.1804\n",
      "epoch : 8, train loss : 0.3103, valid loss : 0.1826\n",
      "epoch : 9, train loss : 0.2953, valid loss : 0.1208\n",
      "epoch : 10, train loss : 0.2690, valid loss : 0.1125\n",
      "epoch : 11, train loss : 0.2734, valid loss : 0.1712\n",
      "epoch : 12, train loss : 0.2276, valid loss : 0.2190\n",
      "epoch : 13, train loss : 0.2434, valid loss : 0.1049\n",
      "epoch : 14, train loss : 0.2331, valid loss : 0.1295\n",
      "epoch : 15, train loss : 0.2187, valid loss : 0.1262\n",
      "epoch : 16, train loss : 0.2117, valid loss : 0.1176\n",
      "epoch : 17, train loss : 0.2330, valid loss : 0.0494\n",
      "epoch : 18, train loss : 0.2212, valid loss : 0.0651\n",
      "epoch : 19, train loss : 0.2381, valid loss : 0.1069\n",
      "epoch : 20, train loss : 0.2023, valid loss : 0.1438\n",
      "epoch : 21, train loss : 0.2065, valid loss : 0.1153\n",
      "epoch : 22, train loss : 0.2238, valid loss : 0.1380\n",
      "epoch : 23, train loss : 0.2224, valid loss : 0.1163\n",
      "epoch : 24, train loss : 0.2063, valid loss : 0.0870\n",
      "epoch : 25, train loss : 0.2142, valid loss : 0.0557\n",
      "epoch : 26, train loss : 0.1971, valid loss : 0.0367\n",
      "epoch : 27, train loss : 0.2136, valid loss : 0.1563\n",
      "epoch : 28, train loss : 0.2047, valid loss : 0.1243\n",
      "epoch : 29, train loss : 0.2173, valid loss : 0.1089\n",
      "epoch : 30, train loss : 0.2476, valid loss : 0.1289\n",
      "epoch : 31, train loss : 0.2120, valid loss : 0.1700\n",
      "epoch : 32, train loss : 0.2002, valid loss : 0.1051\n",
      "epoch : 33, train loss : 0.2217, valid loss : 0.1378\n",
      "epoch : 34, train loss : 0.2169, valid loss : 0.0995\n",
      "epoch : 35, train loss : 0.1956, valid loss : 0.1807\n",
      "epoch : 36, train loss : 0.2106, valid loss : 0.1043\n",
      "epoch : 37, train loss : 0.2209, valid loss : 0.1265\n",
      "epoch : 38, train loss : 0.2148, valid loss : 0.1228\n",
      "epoch : 39, train loss : 0.2053, valid loss : 0.1130\n",
      "epoch : 40, train loss : 0.2100, valid loss : 0.0807\n",
      "epoch : 41, train loss : 0.2129, valid loss : 0.1303\n",
      "epoch : 42, train loss : 0.2165, valid loss : 0.1244\n",
      "epoch : 43, train loss : 0.1838, valid loss : 0.0932\n",
      "epoch : 44, train loss : 0.2216, valid loss : 0.1564\n",
      "epoch : 45, train loss : 0.1972, valid loss : 0.1176\n",
      "epoch : 46, train loss : 0.1835, valid loss : 0.1423\n",
      "epoch : 47, train loss : 0.2077, valid loss : 0.0935\n",
      "epoch : 48, train loss : 0.1884, valid loss : 0.2293\n",
      "epoch : 49, train loss : 0.1951, valid loss : 0.0784\n",
      "epoch : 50, train loss : 0.2000, valid loss : 0.1897\n",
      "epoch : 51, train loss : 0.1879, valid loss : 0.0755\n",
      "epoch : 52, train loss : 0.1878, valid loss : 0.1297\n",
      "epoch : 53, train loss : 0.1837, valid loss : 0.0742\n",
      "epoch : 54, train loss : 0.2074, valid loss : 0.1837\n",
      "epoch : 55, train loss : 0.2020, valid loss : 0.1259\n",
      "epoch : 56, train loss : 0.1803, valid loss : 0.1110\n",
      "epoch : 57, train loss : 0.1990, valid loss : 0.1070\n",
      "epoch : 58, train loss : 0.2165, valid loss : 0.1494\n",
      "epoch : 59, train loss : 0.1770, valid loss : 0.1455\n",
      "epoch : 60, train loss : 0.2024, valid loss : 0.1156\n",
      "epoch : 61, train loss : 0.1971, valid loss : 0.1720\n",
      "epoch : 62, train loss : 0.1875, valid loss : 0.1497\n",
      "epoch : 63, train loss : 0.1998, valid loss : 0.0653\n",
      "epoch : 64, train loss : 0.1850, valid loss : 0.1032\n",
      "epoch : 65, train loss : 0.1958, valid loss : 0.0462\n",
      "epoch : 66, train loss : 0.1964, valid loss : 0.1461\n",
      "epoch : 67, train loss : 0.2000, valid loss : 0.0796\n",
      "epoch : 68, train loss : 0.1860, valid loss : 0.1177\n",
      "epoch : 69, train loss : 0.1883, valid loss : 0.1107\n",
      "epoch : 70, train loss : 0.2101, valid loss : 0.1427\n",
      "epoch : 71, train loss : 0.1955, valid loss : 0.1229\n",
      "epoch : 72, train loss : 0.1692, valid loss : 0.1036\n",
      "epoch : 73, train loss : 0.1850, valid loss : 0.0791\n",
      "epoch : 74, train loss : 0.1916, valid loss : 0.1628\n",
      "epoch : 75, train loss : 0.2053, valid loss : 0.0937\n",
      "epoch : 76, train loss : 0.2197, valid loss : 0.0452\n",
      "epoch : 77, train loss : 0.1899, valid loss : 0.1043\n",
      "epoch : 78, train loss : 0.1814, valid loss : 0.0974\n",
      "epoch : 79, train loss : 0.2045, valid loss : 0.1177\n",
      "epoch : 80, train loss : 0.1956, valid loss : 0.1622\n",
      "epoch : 81, train loss : 0.2064, valid loss : 0.0902\n",
      "epoch : 82, train loss : 0.1992, valid loss : 0.1173\n",
      "epoch : 83, train loss : 0.1936, valid loss : 0.1146\n",
      "epoch : 84, train loss : 0.1933, valid loss : 0.1947\n",
      "epoch : 85, train loss : 0.1787, valid loss : 0.1205\n",
      "epoch : 86, train loss : 0.1932, valid loss : 0.1081\n",
      "epoch : 87, train loss : 0.1902, valid loss : 0.1103\n",
      "epoch : 88, train loss : 0.2064, valid loss : 0.1924\n",
      "epoch : 89, train loss : 0.1847, valid loss : 0.0765\n",
      "epoch : 90, train loss : 0.1934, valid loss : 0.1564\n",
      "epoch : 91, train loss : 0.1749, valid loss : 0.1102\n",
      "epoch : 92, train loss : 0.1898, valid loss : 0.1164\n",
      "epoch : 93, train loss : 0.1786, valid loss : 0.1216\n",
      "epoch : 94, train loss : 0.1927, valid loss : 0.1264\n",
      "epoch : 95, train loss : 0.2022, valid loss : 0.1293\n",
      "epoch : 96, train loss : 0.2013, valid loss : 0.1320\n",
      "epoch : 97, train loss : 0.1914, valid loss : 0.1132\n",
      "epoch : 98, train loss : 0.1866, valid loss : 0.2041\n",
      "epoch : 99, train loss : 0.1999, valid loss : 0.1346\n",
      "epoch : 100, train loss : 0.1914, valid loss : 0.0689\n",
      "epoch : 101, train loss : 0.1853, valid loss : 0.0635\n",
      "epoch : 102, train loss : 0.2064, valid loss : 0.1530\n",
      "epoch : 103, train loss : 0.1953, valid loss : 0.0785\n",
      "epoch : 104, train loss : 0.2060, valid loss : 0.1109\n",
      "epoch : 105, train loss : 0.2108, valid loss : 0.1249\n",
      "epoch : 106, train loss : 0.1866, valid loss : 0.0801\n",
      "epoch : 107, train loss : 0.1779, valid loss : 0.1020\n",
      "epoch : 108, train loss : 0.1843, valid loss : 0.1010\n",
      "epoch : 109, train loss : 0.1897, valid loss : 0.1313\n",
      "epoch : 110, train loss : 0.1979, valid loss : 0.1358\n",
      "epoch : 111, train loss : 0.2110, valid loss : 0.1406\n",
      "epoch : 112, train loss : 0.1953, valid loss : 0.1204\n",
      "epoch : 113, train loss : 0.1790, valid loss : 0.1091\n",
      "epoch : 114, train loss : 0.1763, valid loss : 0.0843\n",
      "epoch : 115, train loss : 0.2001, valid loss : 0.0753\n",
      "epoch : 116, train loss : 0.1796, valid loss : 0.0832\n",
      "epoch : 117, train loss : 0.1936, valid loss : 0.1094\n",
      "epoch : 118, train loss : 0.1899, valid loss : 0.1108\n",
      "epoch : 119, train loss : 0.1658, valid loss : 0.1332\n",
      "epoch : 120, train loss : 0.1870, valid loss : 0.0915\n",
      "epoch : 121, train loss : 0.1989, valid loss : 0.1479\n",
      "epoch : 122, train loss : 0.1914, valid loss : 0.1240\n",
      "epoch : 123, train loss : 0.1848, valid loss : 0.1494\n",
      "epoch : 124, train loss : 0.1865, valid loss : 0.0844\n",
      "epoch : 125, train loss : 0.1859, valid loss : 0.0778\n",
      "epoch : 126, train loss : 0.1838, valid loss : 0.0804\n",
      "epoch : 127, train loss : 0.1893, valid loss : 0.1182\n",
      "epoch : 128, train loss : 0.1769, valid loss : 0.0873\n",
      "epoch : 129, train loss : 0.1929, valid loss : 0.0320\n",
      "epoch : 130, train loss : 0.1827, valid loss : 0.1348\n",
      "epoch : 131, train loss : 0.1895, valid loss : 0.1175\n",
      "epoch : 132, train loss : 0.2019, valid loss : 0.1554\n",
      "epoch : 133, train loss : 0.1854, valid loss : 0.1453\n",
      "epoch : 134, train loss : 0.2011, valid loss : 0.1229\n",
      "epoch : 135, train loss : 0.1841, valid loss : 0.0757\n",
      "epoch : 136, train loss : 0.1884, valid loss : 0.0903\n",
      "epoch : 137, train loss : 0.1961, valid loss : 0.1324\n",
      "epoch : 138, train loss : 0.1674, valid loss : 0.1888\n",
      "epoch : 139, train loss : 0.1962, valid loss : 0.1708\n",
      "epoch : 140, train loss : 0.1801, valid loss : 0.1127\n",
      "epoch : 141, train loss : 0.1827, valid loss : 0.1304\n",
      "epoch : 142, train loss : 0.1873, valid loss : 0.0971\n",
      "epoch : 143, train loss : 0.1903, valid loss : 0.0992\n",
      "epoch : 144, train loss : 0.1912, valid loss : 0.1266\n",
      "epoch : 145, train loss : 0.1841, valid loss : 0.1149\n",
      "epoch : 146, train loss : 0.1811, valid loss : 0.1155\n",
      "epoch : 147, train loss : 0.1865, valid loss : 0.0746\n",
      "epoch : 148, train loss : 0.1937, valid loss : 0.1845\n",
      "epoch : 149, train loss : 0.1756, valid loss : 0.1631\n",
      "epoch : 150, train loss : 0.1911, valid loss : 0.0753\n",
      "epoch : 151, train loss : 0.1884, valid loss : 0.1146\n",
      "epoch : 152, train loss : 0.1806, valid loss : 0.1076\n",
      "epoch : 153, train loss : 0.1914, valid loss : 0.0937\n",
      "epoch : 154, train loss : 0.1949, valid loss : 0.0894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 155, train loss : 0.1939, valid loss : 0.0827\n",
      "epoch : 156, train loss : 0.1813, valid loss : 0.1150\n",
      "epoch : 157, train loss : 0.1853, valid loss : 0.0894\n",
      "epoch : 158, train loss : 0.1658, valid loss : 0.1447\n",
      "epoch : 159, train loss : 0.1643, valid loss : 0.0641\n",
      "epoch : 160, train loss : 0.1794, valid loss : 0.1722\n",
      "epoch : 161, train loss : 0.1671, valid loss : 0.0734\n",
      "epoch : 162, train loss : 0.1806, valid loss : 0.1215\n",
      "epoch : 163, train loss : 0.1750, valid loss : 0.0511\n",
      "epoch : 164, train loss : 0.1780, valid loss : 0.1300\n",
      "epoch : 165, train loss : 0.1614, valid loss : 0.1196\n",
      "epoch : 166, train loss : 0.1657, valid loss : 0.0992\n",
      "epoch : 167, train loss : 0.1966, valid loss : 0.1223\n",
      "epoch : 168, train loss : 0.1926, valid loss : 0.1483\n",
      "epoch : 169, train loss : 0.1725, valid loss : 0.0836\n",
      "epoch : 170, train loss : 0.1865, valid loss : 0.1141\n",
      "epoch : 171, train loss : 0.1808, valid loss : 0.0648\n",
      "epoch : 172, train loss : 0.1640, valid loss : 0.1142\n",
      "epoch : 173, train loss : 0.1847, valid loss : 0.0577\n",
      "epoch : 174, train loss : 0.1830, valid loss : 0.1241\n",
      "epoch : 175, train loss : 0.1881, valid loss : 0.0853\n",
      "epoch : 176, train loss : 0.1850, valid loss : 0.0897\n",
      "epoch : 177, train loss : 0.1716, valid loss : 0.1177\n",
      "epoch : 178, train loss : 0.1888, valid loss : 0.1126\n",
      "epoch : 179, train loss : 0.1755, valid loss : 0.0866\n",
      "epoch : 180, train loss : 0.1923, valid loss : 0.1103\n",
      "epoch : 181, train loss : 0.1784, valid loss : 0.1491\n",
      "epoch : 182, train loss : 0.1788, valid loss : 0.1425\n",
      "epoch : 183, train loss : 0.1904, valid loss : 0.1807\n",
      "epoch : 184, train loss : 0.1914, valid loss : 0.1096\n",
      "epoch : 185, train loss : 0.1792, valid loss : 0.0944\n",
      "epoch : 186, train loss : 0.1891, valid loss : 0.1055\n",
      "epoch : 187, train loss : 0.1876, valid loss : 0.0588\n",
      "epoch : 188, train loss : 0.1680, valid loss : 0.0895\n",
      "epoch : 189, train loss : 0.1637, valid loss : 0.1173\n",
      "epoch : 190, train loss : 0.1638, valid loss : 0.0935\n",
      "epoch : 191, train loss : 0.1881, valid loss : 0.1492\n",
      "epoch : 192, train loss : 0.1806, valid loss : 0.1332\n",
      "epoch : 193, train loss : 0.1938, valid loss : 0.1066\n",
      "epoch : 194, train loss : 0.1790, valid loss : 0.0873\n",
      "epoch : 195, train loss : 0.1729, valid loss : 0.1398\n",
      "epoch : 196, train loss : 0.1711, valid loss : 0.1351\n",
      "epoch : 197, train loss : 0.1615, valid loss : 0.0864\n",
      "epoch : 198, train loss : 0.1656, valid loss : 0.1134\n",
      "epoch : 199, train loss : 0.1766, valid loss : 0.1207\n",
      "epoch : 200, train loss : 0.1719, valid loss : 0.1060\n"
     ]
    }
   ],
   "source": [
    "mean_train_losses = []\n",
    "mean_valid_losses = []\n",
    "valid_acc_list = []\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for i, (embeddings, labels) in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(embeddings)\n",
    "        \n",
    "        #print(\"Embeddings: \", embeddings, \" Output: \", outputs, \" Labels: \", labels.view(-1,1))\n",
    "        \n",
    "        loss = torch.sqrt(loss_fn(outputs, labels.view(-1,1)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "            \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (embeddings, labels) in enumerate(valid_loader):\n",
    "            outputs = model(embeddings)\n",
    "            loss = torch.sqrt(loss_fn(outputs, labels.view(-1,1)))\n",
    "            \n",
    "            valid_losses.append(loss.item())\n",
    "            \n",
    "    mean_train_losses.append(np.mean(train_losses))\n",
    "    mean_valid_losses.append(np.mean(valid_losses))\n",
    "    \n",
    "    print('epoch : {}, train loss : {:.4f}, valid loss : {:.4f}'\\\n",
    "         .format(epoch+1, np.mean(train_losses), np.mean(valid_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T04:32:41.568398Z",
     "start_time": "2021-12-12T04:32:41.562240Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/func_graph2vec_MLP_with_dropout_543_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T04:32:41.864596Z",
     "start_time": "2021-12-12T04:32:41.572341Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(mean_train_losses, label='train')\n",
    "ax.plot(mean_valid_losses, label='valid')\n",
    "lines, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(lines, labels, loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T04:34:07.132523Z",
     "start_time": "2021-12-12T04:34:06.272934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average test loss: 0.7266895941325596\n",
      "average valid loss: 0.11933957296290569\n",
      "average train loss: 0.20676025260169553\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_losses = []\n",
    "for i, (embeddings, labels) in enumerate(test_loader):\n",
    "    \n",
    "    pred = model(embeddings)\n",
    "    loss = torch.sqrt(loss_fn(pred, labels.view(-1,1)))\n",
    "    #print(\"Prediction:\", pred.detach().numpy(), \" Ground Truth:\", labels.view(-1,1))\n",
    "    test_losses.append(loss.item())\n",
    "print(\"average test loss:\", np.mean(test_losses))\n",
    "print(\"average valid loss:\", np.mean(mean_valid_losses))\n",
    "print(\"average train loss:\", np.mean(mean_train_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dropout - 0.5 0.4 0.3 ***\n",
    "average test loss : 0.7170015105179378\n",
    "average valid loss: 0.11933957296290569\n",
    "average train loss: 0.20676025260169553\n",
    "\n",
    "### dropout - 0.5 0.5 0.5\n",
    "average test loss: 0.732521082673754\n",
    "average valid loss: 0.12119727909032788\n",
    "average train loss: 0.22825816216158137\n",
    "\n",
    "------------------------------------------------------\n",
    "### dropout - 0.5 0.3 0.3\n",
    "average test loss: 0.7378307580947876\n",
    "\n",
    "### dropout - 0.5 0.3 0.4\n",
    "average test loss: 0.8221799305507115\n",
    "\n",
    "### dropout - 0.5 0.4 0.4\n",
    "average test loss: 0.7620049544743129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
