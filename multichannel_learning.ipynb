{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T03:56:58.431790Z",
     "start_time": "2021-12-12T03:56:56.643473Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T03:56:58.441424Z",
     "start_time": "2021-12-12T03:56:58.433941Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "HIDDEN_SIZE = 256\n",
    "LOAD_MODEL = False\n",
    "USE_TRANSFER_LEARNING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T03:56:58.450548Z",
     "start_time": "2021-12-12T03:56:58.444173Z"
    }
   },
   "outputs": [],
   "source": [
    "class Graph2VecEmbeddingsDataset(Dataset):\n",
    "    \"\"\"Graph2Vec Embeddings dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, embeddings, labels=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings (pd.dataframe): Pandas Dataframe with the graph2vec embeddings\n",
    "            labels : Labels indicating intelligence for the respective individual\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        sample = torch.tensor(self.embeddings.iloc[idx]).float()\n",
    "\n",
    "        if self.labels is not None:\n",
    "            return sample, torch.tensor(self.labels.iloc[idx]).float()\n",
    "        else:\n",
    "            return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_y = pd.read_csv(\"intelligence_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subjectkey</th>\n",
       "      <th>nihtbx_totalcomp_uncorrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NDARINV003RTV85</td>\n",
       "      <td>-0.790265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDARINV007W6H7B</td>\n",
       "      <td>-0.571433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NDARINV00BD7VDC</td>\n",
       "      <td>0.632147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NDARINV00LJVZK2</td>\n",
       "      <td>0.194482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NDARINV00NPMHND</td>\n",
       "      <td>-0.680849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6907</th>\n",
       "      <td>NDARINVZZL0VA2F</td>\n",
       "      <td>-0.680849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6908</th>\n",
       "      <td>NDARINVZZLZCKAY</td>\n",
       "      <td>-0.462016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6909</th>\n",
       "      <td>NDARINVZZPKBDAC</td>\n",
       "      <td>-0.680849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6910</th>\n",
       "      <td>NDARINVZZZ2ALR6</td>\n",
       "      <td>-0.243184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6911</th>\n",
       "      <td>NDARINVZZZNB0XC</td>\n",
       "      <td>0.632147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6912 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           subjectkey  nihtbx_totalcomp_uncorrected\n",
       "0     NDARINV003RTV85                     -0.790265\n",
       "1     NDARINV007W6H7B                     -0.571433\n",
       "2     NDARINV00BD7VDC                      0.632147\n",
       "3     NDARINV00LJVZK2                      0.194482\n",
       "4     NDARINV00NPMHND                     -0.680849\n",
       "...               ...                           ...\n",
       "6907  NDARINVZZL0VA2F                     -0.680849\n",
       "6908  NDARINVZZLZCKAY                     -0.462016\n",
       "6909  NDARINVZZPKBDAC                     -0.680849\n",
       "6910  NDARINVZZZ2ALR6                     -0.243184\n",
       "6911  NDARINVZZZNB0XC                      0.632147\n",
       "\n",
       "[6912 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T03:56:58.543231Z",
     "start_time": "2021-12-12T03:56:58.452256Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read data, make fake y and drop the \"type\" column\n",
    "\n",
    "#data = pd.read_csv(\"data/nci1.csv\")\n",
    "#data['y'] = np.random.normal(100,30, size=len(data))\n",
    "#data = data.drop(\"type\", axis=1)\n",
    "\n",
    "data = pd.read_csv(\"features/struct_func_embedding.csv\")\n",
    "#cols_to_norm = data.columns#.drop(\"y\")\n",
    "#data[cols_to_norm]=(data[cols_to_norm]-data[cols_to_norm].mean())/data[cols_to_norm].std()\n",
    "#data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjid=data['type'].values[9].split('/')[-1].split('_')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NDARINV040B4TRC'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['type'].values[9][-15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/Stella/MLVU_multimodality/graph2vec/structural_graph_for_graph2vec_1212/graph2vec_structural_graph_NDARINV003RTV85'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['type'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NDARINV040B4TRC'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NDARINV003RTV85'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_y['subjectkey'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for i in data['type']:\n",
    "    subjid = i.split('/')[-1].split('_')[-1]\n",
    "    for j in range(len(raw_y)):\n",
    "        if subjid == raw_y['subjectkey'][j]:\n",
    "            y.append(raw_y['nihtbx_totalcomp_uncorrected'][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2085"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>x_0</th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>x_6</th>\n",
       "      <th>x_7</th>\n",
       "      <th>x_8</th>\n",
       "      <th>...</th>\n",
       "      <th>x_118.1</th>\n",
       "      <th>x_119.1</th>\n",
       "      <th>x_120.1</th>\n",
       "      <th>x_121.1</th>\n",
       "      <th>x_122.1</th>\n",
       "      <th>x_123.1</th>\n",
       "      <th>x_124.1</th>\n",
       "      <th>x_125.1</th>\n",
       "      <th>x_126.1</th>\n",
       "      <th>x_127.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/ubuntu/Stella/MLVU_multimodality/graph2v...</td>\n",
       "      <td>-0.060564</td>\n",
       "      <td>-0.090097</td>\n",
       "      <td>0.017879</td>\n",
       "      <td>-0.233405</td>\n",
       "      <td>-0.031025</td>\n",
       "      <td>-0.161997</td>\n",
       "      <td>-0.110498</td>\n",
       "      <td>-0.106946</td>\n",
       "      <td>0.161249</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058875</td>\n",
       "      <td>-0.045145</td>\n",
       "      <td>-0.035233</td>\n",
       "      <td>0.033590</td>\n",
       "      <td>-0.003432</td>\n",
       "      <td>0.033996</td>\n",
       "      <td>0.029607</td>\n",
       "      <td>0.008686</td>\n",
       "      <td>-0.028766</td>\n",
       "      <td>0.067319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/ubuntu/Stella/MLVU_multimodality/graph2v...</td>\n",
       "      <td>-0.058420</td>\n",
       "      <td>-0.086559</td>\n",
       "      <td>0.019345</td>\n",
       "      <td>-0.226949</td>\n",
       "      <td>-0.031319</td>\n",
       "      <td>-0.155380</td>\n",
       "      <td>-0.109569</td>\n",
       "      <td>-0.100978</td>\n",
       "      <td>0.149402</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024165</td>\n",
       "      <td>-0.021222</td>\n",
       "      <td>-0.019916</td>\n",
       "      <td>0.022018</td>\n",
       "      <td>0.003037</td>\n",
       "      <td>0.020805</td>\n",
       "      <td>0.014551</td>\n",
       "      <td>0.007052</td>\n",
       "      <td>-0.009735</td>\n",
       "      <td>0.033356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/ubuntu/Stella/MLVU_multimodality/graph2v...</td>\n",
       "      <td>-0.066414</td>\n",
       "      <td>-0.084501</td>\n",
       "      <td>0.019682</td>\n",
       "      <td>-0.234007</td>\n",
       "      <td>-0.033996</td>\n",
       "      <td>-0.160188</td>\n",
       "      <td>-0.116341</td>\n",
       "      <td>-0.105766</td>\n",
       "      <td>0.160227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.150266</td>\n",
       "      <td>-0.102514</td>\n",
       "      <td>-0.109775</td>\n",
       "      <td>0.158490</td>\n",
       "      <td>-0.032627</td>\n",
       "      <td>0.073411</td>\n",
       "      <td>0.106722</td>\n",
       "      <td>0.041687</td>\n",
       "      <td>-0.089557</td>\n",
       "      <td>0.197947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/ubuntu/Stella/MLVU_multimodality/graph2v...</td>\n",
       "      <td>-0.061569</td>\n",
       "      <td>-0.079573</td>\n",
       "      <td>0.018303</td>\n",
       "      <td>-0.221120</td>\n",
       "      <td>-0.028189</td>\n",
       "      <td>-0.150833</td>\n",
       "      <td>-0.103980</td>\n",
       "      <td>-0.096945</td>\n",
       "      <td>0.152231</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.288406</td>\n",
       "      <td>-0.231228</td>\n",
       "      <td>-0.167932</td>\n",
       "      <td>0.187091</td>\n",
       "      <td>-0.006038</td>\n",
       "      <td>0.168102</td>\n",
       "      <td>0.160195</td>\n",
       "      <td>0.068691</td>\n",
       "      <td>-0.139464</td>\n",
       "      <td>0.327302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/ubuntu/Stella/MLVU_multimodality/graph2v...</td>\n",
       "      <td>-0.063210</td>\n",
       "      <td>-0.087080</td>\n",
       "      <td>0.025141</td>\n",
       "      <td>-0.234608</td>\n",
       "      <td>-0.028360</td>\n",
       "      <td>-0.162248</td>\n",
       "      <td>-0.116670</td>\n",
       "      <td>-0.104763</td>\n",
       "      <td>0.154640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030238</td>\n",
       "      <td>0.018195</td>\n",
       "      <td>0.016880</td>\n",
       "      <td>-0.020034</td>\n",
       "      <td>0.002513</td>\n",
       "      <td>-0.019045</td>\n",
       "      <td>-0.013067</td>\n",
       "      <td>-0.003425</td>\n",
       "      <td>0.016418</td>\n",
       "      <td>-0.028255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080</th>\n",
       "      <td>/home/ubuntu/Stella/MLVU_multimodality/graph2v...</td>\n",
       "      <td>-0.052150</td>\n",
       "      <td>-0.077419</td>\n",
       "      <td>0.014720</td>\n",
       "      <td>-0.209189</td>\n",
       "      <td>-0.026127</td>\n",
       "      <td>-0.145545</td>\n",
       "      <td>-0.099978</td>\n",
       "      <td>-0.094635</td>\n",
       "      <td>0.141908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019741</td>\n",
       "      <td>0.016145</td>\n",
       "      <td>0.010044</td>\n",
       "      <td>-0.016801</td>\n",
       "      <td>-0.002833</td>\n",
       "      <td>-0.017898</td>\n",
       "      <td>-0.013662</td>\n",
       "      <td>-0.002859</td>\n",
       "      <td>0.011401</td>\n",
       "      <td>-0.028907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2081</th>\n",
       "      <td>/home/ubuntu/Stella/MLVU_multimodality/graph2v...</td>\n",
       "      <td>-0.056559</td>\n",
       "      <td>-0.080355</td>\n",
       "      <td>0.022327</td>\n",
       "      <td>-0.229628</td>\n",
       "      <td>-0.029295</td>\n",
       "      <td>-0.160573</td>\n",
       "      <td>-0.113333</td>\n",
       "      <td>-0.101679</td>\n",
       "      <td>0.155368</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042761</td>\n",
       "      <td>-0.035346</td>\n",
       "      <td>-0.021141</td>\n",
       "      <td>0.032329</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>0.024632</td>\n",
       "      <td>0.020912</td>\n",
       "      <td>0.010401</td>\n",
       "      <td>-0.017409</td>\n",
       "      <td>0.050803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2082</th>\n",
       "      <td>/home/ubuntu/Stella/MLVU_multimodality/graph2v...</td>\n",
       "      <td>-0.059904</td>\n",
       "      <td>-0.084092</td>\n",
       "      <td>0.022182</td>\n",
       "      <td>-0.218934</td>\n",
       "      <td>-0.033377</td>\n",
       "      <td>-0.157618</td>\n",
       "      <td>-0.110358</td>\n",
       "      <td>-0.098928</td>\n",
       "      <td>0.148119</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280970</td>\n",
       "      <td>-0.237530</td>\n",
       "      <td>-0.150155</td>\n",
       "      <td>0.182934</td>\n",
       "      <td>-0.006497</td>\n",
       "      <td>0.166452</td>\n",
       "      <td>0.146688</td>\n",
       "      <td>0.059213</td>\n",
       "      <td>-0.125576</td>\n",
       "      <td>0.341123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>/home/ubuntu/Stella/MLVU_multimodality/graph2v...</td>\n",
       "      <td>-0.062419</td>\n",
       "      <td>-0.086961</td>\n",
       "      <td>0.018969</td>\n",
       "      <td>-0.241844</td>\n",
       "      <td>-0.030937</td>\n",
       "      <td>-0.168730</td>\n",
       "      <td>-0.120848</td>\n",
       "      <td>-0.102684</td>\n",
       "      <td>0.162021</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.223731</td>\n",
       "      <td>-0.166204</td>\n",
       "      <td>-0.126386</td>\n",
       "      <td>0.149962</td>\n",
       "      <td>-0.006443</td>\n",
       "      <td>0.134264</td>\n",
       "      <td>0.123120</td>\n",
       "      <td>0.053563</td>\n",
       "      <td>-0.104671</td>\n",
       "      <td>0.250376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>/home/ubuntu/Stella/MLVU_multimodality/graph2v...</td>\n",
       "      <td>-0.061167</td>\n",
       "      <td>-0.087081</td>\n",
       "      <td>0.017005</td>\n",
       "      <td>-0.221845</td>\n",
       "      <td>-0.031672</td>\n",
       "      <td>-0.159279</td>\n",
       "      <td>-0.111235</td>\n",
       "      <td>-0.100327</td>\n",
       "      <td>0.156237</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.222346</td>\n",
       "      <td>-0.180038</td>\n",
       "      <td>-0.134196</td>\n",
       "      <td>0.146822</td>\n",
       "      <td>-0.007504</td>\n",
       "      <td>0.139472</td>\n",
       "      <td>0.127322</td>\n",
       "      <td>0.045819</td>\n",
       "      <td>-0.116230</td>\n",
       "      <td>0.263145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2085 rows Ã— 257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   type       x_0       x_1  \\\n",
       "0     /home/ubuntu/Stella/MLVU_multimodality/graph2v... -0.060564 -0.090097   \n",
       "1     /home/ubuntu/Stella/MLVU_multimodality/graph2v... -0.058420 -0.086559   \n",
       "2     /home/ubuntu/Stella/MLVU_multimodality/graph2v... -0.066414 -0.084501   \n",
       "3     /home/ubuntu/Stella/MLVU_multimodality/graph2v... -0.061569 -0.079573   \n",
       "4     /home/ubuntu/Stella/MLVU_multimodality/graph2v... -0.063210 -0.087080   \n",
       "...                                                 ...       ...       ...   \n",
       "2080  /home/ubuntu/Stella/MLVU_multimodality/graph2v... -0.052150 -0.077419   \n",
       "2081  /home/ubuntu/Stella/MLVU_multimodality/graph2v... -0.056559 -0.080355   \n",
       "2082  /home/ubuntu/Stella/MLVU_multimodality/graph2v... -0.059904 -0.084092   \n",
       "2083  /home/ubuntu/Stella/MLVU_multimodality/graph2v... -0.062419 -0.086961   \n",
       "2084  /home/ubuntu/Stella/MLVU_multimodality/graph2v... -0.061167 -0.087081   \n",
       "\n",
       "           x_2       x_3       x_4       x_5       x_6       x_7       x_8  \\\n",
       "0     0.017879 -0.233405 -0.031025 -0.161997 -0.110498 -0.106946  0.161249   \n",
       "1     0.019345 -0.226949 -0.031319 -0.155380 -0.109569 -0.100978  0.149402   \n",
       "2     0.019682 -0.234007 -0.033996 -0.160188 -0.116341 -0.105766  0.160227   \n",
       "3     0.018303 -0.221120 -0.028189 -0.150833 -0.103980 -0.096945  0.152231   \n",
       "4     0.025141 -0.234608 -0.028360 -0.162248 -0.116670 -0.104763  0.154640   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2080  0.014720 -0.209189 -0.026127 -0.145545 -0.099978 -0.094635  0.141908   \n",
       "2081  0.022327 -0.229628 -0.029295 -0.160573 -0.113333 -0.101679  0.155368   \n",
       "2082  0.022182 -0.218934 -0.033377 -0.157618 -0.110358 -0.098928  0.148119   \n",
       "2083  0.018969 -0.241844 -0.030937 -0.168730 -0.120848 -0.102684  0.162021   \n",
       "2084  0.017005 -0.221845 -0.031672 -0.159279 -0.111235 -0.100327  0.156237   \n",
       "\n",
       "      ...   x_118.1   x_119.1   x_120.1   x_121.1   x_122.1   x_123.1  \\\n",
       "0     ... -0.058875 -0.045145 -0.035233  0.033590 -0.003432  0.033996   \n",
       "1     ... -0.024165 -0.021222 -0.019916  0.022018  0.003037  0.020805   \n",
       "2     ... -0.150266 -0.102514 -0.109775  0.158490 -0.032627  0.073411   \n",
       "3     ... -0.288406 -0.231228 -0.167932  0.187091 -0.006038  0.168102   \n",
       "4     ...  0.030238  0.018195  0.016880 -0.020034  0.002513 -0.019045   \n",
       "...   ...       ...       ...       ...       ...       ...       ...   \n",
       "2080  ...  0.019741  0.016145  0.010044 -0.016801 -0.002833 -0.017898   \n",
       "2081  ... -0.042761 -0.035346 -0.021141  0.032329  0.001603  0.024632   \n",
       "2082  ... -0.280970 -0.237530 -0.150155  0.182934 -0.006497  0.166452   \n",
       "2083  ... -0.223731 -0.166204 -0.126386  0.149962 -0.006443  0.134264   \n",
       "2084  ... -0.222346 -0.180038 -0.134196  0.146822 -0.007504  0.139472   \n",
       "\n",
       "       x_124.1   x_125.1   x_126.1   x_127.1  \n",
       "0     0.029607  0.008686 -0.028766  0.067319  \n",
       "1     0.014551  0.007052 -0.009735  0.033356  \n",
       "2     0.106722  0.041687 -0.089557  0.197947  \n",
       "3     0.160195  0.068691 -0.139464  0.327302  \n",
       "4    -0.013067 -0.003425  0.016418 -0.028255  \n",
       "...        ...       ...       ...       ...  \n",
       "2080 -0.013662 -0.002859  0.011401 -0.028907  \n",
       "2081  0.020912  0.010401 -0.017409  0.050803  \n",
       "2082  0.146688  0.059213 -0.125576  0.341123  \n",
       "2083  0.123120  0.053563 -0.104671  0.250376  \n",
       "2084  0.127322  0.045819 -0.116230  0.263145  \n",
       "\n",
       "[2085 rows x 257 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['y'] = y\n",
    "data = data.drop(\"type\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_0</th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>x_6</th>\n",
       "      <th>x_7</th>\n",
       "      <th>x_8</th>\n",
       "      <th>x_9</th>\n",
       "      <th>...</th>\n",
       "      <th>x_119.1</th>\n",
       "      <th>x_120.1</th>\n",
       "      <th>x_121.1</th>\n",
       "      <th>x_122.1</th>\n",
       "      <th>x_123.1</th>\n",
       "      <th>x_124.1</th>\n",
       "      <th>x_125.1</th>\n",
       "      <th>x_126.1</th>\n",
       "      <th>x_127.1</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.060564</td>\n",
       "      <td>-0.090097</td>\n",
       "      <td>0.017879</td>\n",
       "      <td>-0.233405</td>\n",
       "      <td>-0.031025</td>\n",
       "      <td>-0.161997</td>\n",
       "      <td>-0.110498</td>\n",
       "      <td>-0.106946</td>\n",
       "      <td>0.161249</td>\n",
       "      <td>-0.466659</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045145</td>\n",
       "      <td>-0.035233</td>\n",
       "      <td>0.033590</td>\n",
       "      <td>-0.003432</td>\n",
       "      <td>0.033996</td>\n",
       "      <td>0.029607</td>\n",
       "      <td>0.008686</td>\n",
       "      <td>-0.028766</td>\n",
       "      <td>0.067319</td>\n",
       "      <td>-0.790265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.058420</td>\n",
       "      <td>-0.086559</td>\n",
       "      <td>0.019345</td>\n",
       "      <td>-0.226949</td>\n",
       "      <td>-0.031319</td>\n",
       "      <td>-0.155380</td>\n",
       "      <td>-0.109569</td>\n",
       "      <td>-0.100978</td>\n",
       "      <td>0.149402</td>\n",
       "      <td>-0.449210</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021222</td>\n",
       "      <td>-0.019916</td>\n",
       "      <td>0.022018</td>\n",
       "      <td>0.003037</td>\n",
       "      <td>0.020805</td>\n",
       "      <td>0.014551</td>\n",
       "      <td>0.007052</td>\n",
       "      <td>-0.009735</td>\n",
       "      <td>0.033356</td>\n",
       "      <td>-0.680849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.066414</td>\n",
       "      <td>-0.084501</td>\n",
       "      <td>0.019682</td>\n",
       "      <td>-0.234007</td>\n",
       "      <td>-0.033996</td>\n",
       "      <td>-0.160188</td>\n",
       "      <td>-0.116341</td>\n",
       "      <td>-0.105766</td>\n",
       "      <td>0.160227</td>\n",
       "      <td>-0.463173</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102514</td>\n",
       "      <td>-0.109775</td>\n",
       "      <td>0.158490</td>\n",
       "      <td>-0.032627</td>\n",
       "      <td>0.073411</td>\n",
       "      <td>0.106722</td>\n",
       "      <td>0.041687</td>\n",
       "      <td>-0.089557</td>\n",
       "      <td>0.197947</td>\n",
       "      <td>0.632147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.061569</td>\n",
       "      <td>-0.079573</td>\n",
       "      <td>0.018303</td>\n",
       "      <td>-0.221120</td>\n",
       "      <td>-0.028189</td>\n",
       "      <td>-0.150833</td>\n",
       "      <td>-0.103980</td>\n",
       "      <td>-0.096945</td>\n",
       "      <td>0.152231</td>\n",
       "      <td>-0.431845</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.231228</td>\n",
       "      <td>-0.167932</td>\n",
       "      <td>0.187091</td>\n",
       "      <td>-0.006038</td>\n",
       "      <td>0.168102</td>\n",
       "      <td>0.160195</td>\n",
       "      <td>0.068691</td>\n",
       "      <td>-0.139464</td>\n",
       "      <td>0.327302</td>\n",
       "      <td>-0.133767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.063210</td>\n",
       "      <td>-0.087080</td>\n",
       "      <td>0.025141</td>\n",
       "      <td>-0.234608</td>\n",
       "      <td>-0.028360</td>\n",
       "      <td>-0.162248</td>\n",
       "      <td>-0.116670</td>\n",
       "      <td>-0.104763</td>\n",
       "      <td>0.154640</td>\n",
       "      <td>-0.459196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018195</td>\n",
       "      <td>0.016880</td>\n",
       "      <td>-0.020034</td>\n",
       "      <td>0.002513</td>\n",
       "      <td>-0.019045</td>\n",
       "      <td>-0.013067</td>\n",
       "      <td>-0.003425</td>\n",
       "      <td>0.016418</td>\n",
       "      <td>-0.028255</td>\n",
       "      <td>-0.243184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080</th>\n",
       "      <td>-0.052150</td>\n",
       "      <td>-0.077419</td>\n",
       "      <td>0.014720</td>\n",
       "      <td>-0.209189</td>\n",
       "      <td>-0.026127</td>\n",
       "      <td>-0.145545</td>\n",
       "      <td>-0.099978</td>\n",
       "      <td>-0.094635</td>\n",
       "      <td>0.141908</td>\n",
       "      <td>-0.413155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016145</td>\n",
       "      <td>0.010044</td>\n",
       "      <td>-0.016801</td>\n",
       "      <td>-0.002833</td>\n",
       "      <td>-0.017898</td>\n",
       "      <td>-0.013662</td>\n",
       "      <td>-0.002859</td>\n",
       "      <td>0.011401</td>\n",
       "      <td>-0.028907</td>\n",
       "      <td>0.194482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2081</th>\n",
       "      <td>-0.056559</td>\n",
       "      <td>-0.080355</td>\n",
       "      <td>0.022327</td>\n",
       "      <td>-0.229628</td>\n",
       "      <td>-0.029295</td>\n",
       "      <td>-0.160573</td>\n",
       "      <td>-0.113333</td>\n",
       "      <td>-0.101679</td>\n",
       "      <td>0.155368</td>\n",
       "      <td>-0.451812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035346</td>\n",
       "      <td>-0.021141</td>\n",
       "      <td>0.032329</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>0.024632</td>\n",
       "      <td>0.020912</td>\n",
       "      <td>0.010401</td>\n",
       "      <td>-0.017409</td>\n",
       "      <td>0.050803</td>\n",
       "      <td>1.179229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2082</th>\n",
       "      <td>-0.059904</td>\n",
       "      <td>-0.084092</td>\n",
       "      <td>0.022182</td>\n",
       "      <td>-0.218934</td>\n",
       "      <td>-0.033377</td>\n",
       "      <td>-0.157618</td>\n",
       "      <td>-0.110358</td>\n",
       "      <td>-0.098928</td>\n",
       "      <td>0.148119</td>\n",
       "      <td>-0.439615</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.237530</td>\n",
       "      <td>-0.150155</td>\n",
       "      <td>0.182934</td>\n",
       "      <td>-0.006497</td>\n",
       "      <td>0.166452</td>\n",
       "      <td>0.146688</td>\n",
       "      <td>0.059213</td>\n",
       "      <td>-0.125576</td>\n",
       "      <td>0.341123</td>\n",
       "      <td>-0.462016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>-0.062419</td>\n",
       "      <td>-0.086961</td>\n",
       "      <td>0.018969</td>\n",
       "      <td>-0.241844</td>\n",
       "      <td>-0.030937</td>\n",
       "      <td>-0.168730</td>\n",
       "      <td>-0.120848</td>\n",
       "      <td>-0.102684</td>\n",
       "      <td>0.162021</td>\n",
       "      <td>-0.471974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.166204</td>\n",
       "      <td>-0.126386</td>\n",
       "      <td>0.149962</td>\n",
       "      <td>-0.006443</td>\n",
       "      <td>0.134264</td>\n",
       "      <td>0.123120</td>\n",
       "      <td>0.053563</td>\n",
       "      <td>-0.104671</td>\n",
       "      <td>0.250376</td>\n",
       "      <td>-0.024351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>-0.061167</td>\n",
       "      <td>-0.087081</td>\n",
       "      <td>0.017005</td>\n",
       "      <td>-0.221845</td>\n",
       "      <td>-0.031672</td>\n",
       "      <td>-0.159279</td>\n",
       "      <td>-0.111235</td>\n",
       "      <td>-0.100327</td>\n",
       "      <td>0.156237</td>\n",
       "      <td>-0.444921</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.180038</td>\n",
       "      <td>-0.134196</td>\n",
       "      <td>0.146822</td>\n",
       "      <td>-0.007504</td>\n",
       "      <td>0.139472</td>\n",
       "      <td>0.127322</td>\n",
       "      <td>0.045819</td>\n",
       "      <td>-0.116230</td>\n",
       "      <td>0.263145</td>\n",
       "      <td>0.632147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2085 rows Ã— 257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x_0       x_1       x_2       x_3       x_4       x_5       x_6  \\\n",
       "0    -0.060564 -0.090097  0.017879 -0.233405 -0.031025 -0.161997 -0.110498   \n",
       "1    -0.058420 -0.086559  0.019345 -0.226949 -0.031319 -0.155380 -0.109569   \n",
       "2    -0.066414 -0.084501  0.019682 -0.234007 -0.033996 -0.160188 -0.116341   \n",
       "3    -0.061569 -0.079573  0.018303 -0.221120 -0.028189 -0.150833 -0.103980   \n",
       "4    -0.063210 -0.087080  0.025141 -0.234608 -0.028360 -0.162248 -0.116670   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2080 -0.052150 -0.077419  0.014720 -0.209189 -0.026127 -0.145545 -0.099978   \n",
       "2081 -0.056559 -0.080355  0.022327 -0.229628 -0.029295 -0.160573 -0.113333   \n",
       "2082 -0.059904 -0.084092  0.022182 -0.218934 -0.033377 -0.157618 -0.110358   \n",
       "2083 -0.062419 -0.086961  0.018969 -0.241844 -0.030937 -0.168730 -0.120848   \n",
       "2084 -0.061167 -0.087081  0.017005 -0.221845 -0.031672 -0.159279 -0.111235   \n",
       "\n",
       "           x_7       x_8       x_9  ...   x_119.1   x_120.1   x_121.1  \\\n",
       "0    -0.106946  0.161249 -0.466659  ... -0.045145 -0.035233  0.033590   \n",
       "1    -0.100978  0.149402 -0.449210  ... -0.021222 -0.019916  0.022018   \n",
       "2    -0.105766  0.160227 -0.463173  ... -0.102514 -0.109775  0.158490   \n",
       "3    -0.096945  0.152231 -0.431845  ... -0.231228 -0.167932  0.187091   \n",
       "4    -0.104763  0.154640 -0.459196  ...  0.018195  0.016880 -0.020034   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2080 -0.094635  0.141908 -0.413155  ...  0.016145  0.010044 -0.016801   \n",
       "2081 -0.101679  0.155368 -0.451812  ... -0.035346 -0.021141  0.032329   \n",
       "2082 -0.098928  0.148119 -0.439615  ... -0.237530 -0.150155  0.182934   \n",
       "2083 -0.102684  0.162021 -0.471974  ... -0.166204 -0.126386  0.149962   \n",
       "2084 -0.100327  0.156237 -0.444921  ... -0.180038 -0.134196  0.146822   \n",
       "\n",
       "       x_122.1   x_123.1   x_124.1   x_125.1   x_126.1   x_127.1         y  \n",
       "0    -0.003432  0.033996  0.029607  0.008686 -0.028766  0.067319 -0.790265  \n",
       "1     0.003037  0.020805  0.014551  0.007052 -0.009735  0.033356 -0.680849  \n",
       "2    -0.032627  0.073411  0.106722  0.041687 -0.089557  0.197947  0.632147  \n",
       "3    -0.006038  0.168102  0.160195  0.068691 -0.139464  0.327302 -0.133767  \n",
       "4     0.002513 -0.019045 -0.013067 -0.003425  0.016418 -0.028255 -0.243184  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "2080 -0.002833 -0.017898 -0.013662 -0.002859  0.011401 -0.028907  0.194482  \n",
       "2081  0.001603  0.024632  0.020912  0.010401 -0.017409  0.050803  1.179229  \n",
       "2082 -0.006497  0.166452  0.146688  0.059213 -0.125576  0.341123 -0.462016  \n",
       "2083 -0.006443  0.134264  0.123120  0.053563 -0.104671  0.250376 -0.024351  \n",
       "2084 -0.007504  0.139472  0.127322  0.045819 -0.116230  0.263145  0.632147  \n",
       "\n",
       "[2085 rows x 257 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T03:56:58.555167Z",
     "start_time": "2021-12-12T03:56:58.544714Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = np.split(data.sample(frac=1, random_state=42), [int(.9*len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T03:56:58.565155Z",
     "start_time": "2021-12-12T03:56:58.557406Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = \\\n",
    "    train_test_split(train_df.iloc[:, 1:], train_df['y'], test_size=1/9, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T03:56:58.575067Z",
     "start_time": "2021-12-12T03:56:58.569068Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test = test_df[\"y\"]\n",
    "X_test = test_df.drop(\"y\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T03:56:58.583376Z",
     "start_time": "2021-12-12T03:56:58.578275Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = Graph2VecEmbeddingsDataset(embeddings=X_train, labels=y_train)\n",
    "valid_dataset = Graph2VecEmbeddingsDataset(embeddings=X_valid, labels=y_valid)\n",
    "test_dataset = Graph2VecEmbeddingsDataset(embeddings=X_test, labels=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T03:56:58.598531Z",
     "start_time": "2021-12-12T03:56:58.585628Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-7.9102e-02,  2.0204e-02, -2.2370e-01, -2.5999e-02, -1.5400e-01,\n",
       "         -1.0596e-01, -9.7772e-02,  1.5631e-01, -4.4454e-01, -1.5771e-01,\n",
       "          1.0530e-01,  1.6490e-01, -1.2046e-01, -1.8206e-02, -7.4203e-02,\n",
       "          1.3171e-01, -1.3311e-02, -3.4454e-02,  1.8862e-02,  8.6579e-02,\n",
       "         -3.6196e-02, -1.5377e-01, -8.7003e-02,  1.1346e-01,  1.4836e-01,\n",
       "         -6.8907e-02,  1.2097e-01, -9.2794e-02, -2.4053e-01,  3.6288e-02,\n",
       "          8.3222e-02,  2.0930e-01,  1.2530e-01,  1.2664e-01,  4.9676e-02,\n",
       "          1.9926e-01,  3.3828e-02,  1.0633e-02, -1.7350e-02, -4.0538e-02,\n",
       "          5.8536e-02, -1.3925e-02, -1.3536e-01,  1.1092e-02,  8.0238e-02,\n",
       "         -8.8536e-02,  3.1406e-02,  1.9980e-01, -1.1908e-01,  5.5592e-02,\n",
       "          3.5060e-02, -2.9032e-02,  1.2752e-01,  6.8406e-02, -5.0526e-02,\n",
       "         -9.7237e-02, -6.5772e-02, -1.8961e-01,  4.7921e-02,  1.2261e-01,\n",
       "          1.3266e-01,  1.1303e-01, -2.9141e-01,  1.5893e-01,  2.8280e-02,\n",
       "         -1.3573e-01,  3.9146e-02,  8.7985e-02,  1.1692e-01, -5.1401e-03,\n",
       "         -2.6615e-02, -9.0168e-02, -1.5832e-01,  2.6219e-02,  8.7988e-02,\n",
       "         -2.0648e-01, -6.7852e-02,  8.6941e-02,  3.1490e-01, -4.3615e-02,\n",
       "         -3.5970e-01, -3.7437e-02, -1.2527e-01,  3.4509e-02, -9.6871e-02,\n",
       "         -1.1000e-01, -1.0591e-01,  9.5383e-02,  5.5028e-02, -1.4551e-01,\n",
       "         -7.2307e-02,  2.1330e-02,  6.1087e-02, -2.9900e-01,  3.1329e-02,\n",
       "         -9.8669e-02,  3.6713e-02, -3.7877e-02, -1.6897e-01, -5.4745e-02,\n",
       "          2.2987e-02,  1.0432e-01, -2.4936e-01,  5.1446e-03,  4.3278e-01,\n",
       "         -1.0451e-02,  7.0511e-02,  1.0226e-01, -2.4953e-02,  8.6635e-03,\n",
       "          3.5745e-02,  1.1788e-01,  6.8036e-03,  1.6729e-03,  8.4024e-02,\n",
       "         -1.1624e-01,  9.2358e-02,  1.4421e-01, -3.3936e-01, -3.9540e-02,\n",
       "          2.5808e-02,  6.8294e-02,  1.8008e-01, -6.5190e-02, -6.9633e-02,\n",
       "         -2.1029e-02, -9.8045e-02,  4.8333e-02,  6.0789e-02, -2.3346e-01,\n",
       "          4.2744e-02,  1.4962e-01,  2.0078e-02, -1.7247e-01,  1.7276e-01,\n",
       "          1.5864e-02,  1.3666e-01,  5.4920e-02, -5.0280e-02, -8.3688e-02,\n",
       "          2.9710e-01,  4.1241e-02,  9.1209e-03, -1.0858e-01, -1.1591e-01,\n",
       "         -3.5099e-03, -8.2184e-02,  4.4909e-02, -1.3621e-02,  2.0660e-02,\n",
       "          4.4805e-02, -5.1467e-02,  1.0313e-01,  5.5481e-02,  1.8245e-01,\n",
       "         -1.8429e-01,  1.2391e-02,  2.5342e-01, -1.0511e-02, -1.4858e-01,\n",
       "          1.5569e-01,  1.0291e-02, -6.8171e-02,  6.1983e-02, -7.4956e-02,\n",
       "          5.4478e-02,  9.3476e-02, -1.7749e-02,  4.4634e-02, -2.3824e-02,\n",
       "         -1.0420e-01,  1.0443e-01,  2.4010e-02,  2.5109e-02,  1.4724e-02,\n",
       "         -6.0827e-02,  1.4823e-01, -1.1879e-01, -3.8111e-02, -1.4845e-01,\n",
       "          4.7469e-02,  2.4806e-01, -1.9702e-01, -1.4839e-01, -3.5616e-02,\n",
       "         -1.8818e-01, -8.5802e-02, -5.2573e-02, -4.3351e-02, -5.9268e-02,\n",
       "          7.1251e-02, -4.2642e-02,  1.1837e-01,  2.7017e-02, -5.0049e-02,\n",
       "         -7.1928e-02,  7.8885e-02, -5.2410e-03,  3.4438e-02,  4.0243e-02,\n",
       "          6.6433e-02, -2.5339e-01,  7.8067e-02, -1.5148e-02,  6.8657e-02,\n",
       "         -3.7408e-02,  9.4100e-02, -2.8020e-03, -1.2439e-01, -1.2846e-01,\n",
       "         -9.7653e-02,  9.9034e-02,  1.8488e-01, -9.3288e-02,  8.2950e-02,\n",
       "         -2.8726e-02,  1.3913e-02,  2.1736e-01,  5.4487e-02,  1.8961e-01,\n",
       "         -8.5250e-02,  6.2293e-02,  1.5522e-01,  1.0023e-01, -2.1651e-01,\n",
       "          1.4394e-01,  1.0464e-01,  5.0560e-02, -3.6077e-02, -2.4139e-02,\n",
       "          2.1488e-04, -3.2406e-01, -2.1567e-01, -1.0292e-01,  1.2648e-01,\n",
       "          7.4258e-02, -4.7615e-02,  2.9536e-02,  1.5876e-01,  1.4553e-01,\n",
       "          7.3387e-02,  2.6739e-02, -1.0006e-01, -1.0447e-01,  5.7914e-03,\n",
       "         -2.1705e-01, -1.8225e-01, -9.2359e-02,  1.5960e-01,  2.3236e-02,\n",
       "          1.2877e-01,  9.2654e-02,  5.2892e-02, -1.4087e-01,  2.9202e-01,\n",
       "         -8.9968e-01]),\n",
       " tensor(-0.8997))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T03:56:58.605780Z",
     "start_time": "2021-12-12T03:56:58.600486Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T03:56:58.644302Z",
     "start_time": "2021-12-12T03:56:58.609040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings shape on PyTroch :  torch.Size([32, 256])\n",
      "labels shape on PyTroch :  torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "embeddings, labels = dataiter.next()\n",
    "\n",
    "print('embeddings shape on PyTroch : ', embeddings.size())\n",
    "print('labels shape on PyTroch : ', labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T03:56:58.652697Z",
     "start_time": "2021-12-12T03:56:58.646160Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0.01)\n",
    "        \n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.layers.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T03:56:58.668977Z",
     "start_time": "2021-12-12T03:56:58.660281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.4, inplace=False)\n",
      "    (8): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (9): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.3, inplace=False)\n",
      "    (12): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MLP(embeddings.size()[1], 256)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T03:56:58.679747Z",
     "start_time": "2021-12-12T03:56:58.671957Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss() #will calculate RMSE next line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T04:32:41.560428Z",
     "start_time": "2021-12-12T03:56:58.683228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1, train loss : 1.2094, valid loss : 0.5184\n",
      "epoch : 2, train loss : 1.2072, valid loss : 0.5349\n",
      "epoch : 3, train loss : 1.1859, valid loss : 0.5646\n",
      "epoch : 4, train loss : 1.2068, valid loss : 0.5349\n",
      "epoch : 5, train loss : 1.2064, valid loss : 0.5623\n",
      "epoch : 6, train loss : 1.2010, valid loss : 0.5559\n",
      "epoch : 7, train loss : 1.1819, valid loss : 0.5418\n",
      "epoch : 8, train loss : 1.1938, valid loss : 0.6033\n",
      "epoch : 9, train loss : 1.1523, valid loss : 0.5535\n",
      "epoch : 10, train loss : 1.1856, valid loss : 0.5437\n",
      "epoch : 11, train loss : 1.2262, valid loss : 0.5099\n",
      "epoch : 12, train loss : 1.1933, valid loss : 0.5519\n",
      "epoch : 13, train loss : 1.2100, valid loss : 0.5701\n",
      "epoch : 14, train loss : 1.1837, valid loss : 0.5635\n",
      "epoch : 15, train loss : 1.1953, valid loss : 0.5767\n",
      "epoch : 16, train loss : 1.2164, valid loss : 0.5977\n",
      "epoch : 17, train loss : 1.2196, valid loss : 0.5625\n",
      "epoch : 18, train loss : 1.2304, valid loss : 0.5311\n",
      "epoch : 19, train loss : 1.2057, valid loss : 0.5619\n",
      "epoch : 20, train loss : 1.2017, valid loss : 0.5684\n",
      "epoch : 21, train loss : 1.1552, valid loss : 0.5722\n",
      "epoch : 22, train loss : 1.1719, valid loss : 0.5730\n",
      "epoch : 23, train loss : 1.2460, valid loss : 0.5791\n",
      "epoch : 24, train loss : 1.1932, valid loss : 0.5832\n",
      "epoch : 25, train loss : 1.2270, valid loss : 0.5392\n",
      "epoch : 26, train loss : 1.1902, valid loss : 0.5577\n",
      "epoch : 27, train loss : 1.1752, valid loss : 0.5723\n",
      "epoch : 28, train loss : 1.1589, valid loss : 0.5748\n",
      "epoch : 29, train loss : 1.1779, valid loss : 0.5570\n",
      "epoch : 30, train loss : 1.1858, valid loss : 0.5454\n",
      "epoch : 31, train loss : 1.1731, valid loss : 0.5291\n",
      "epoch : 32, train loss : 1.2177, valid loss : 0.5502\n",
      "epoch : 33, train loss : 1.2244, valid loss : 0.5739\n",
      "epoch : 34, train loss : 1.2073, valid loss : 0.5812\n",
      "epoch : 35, train loss : 1.2131, valid loss : 0.5763\n",
      "epoch : 36, train loss : 1.1540, valid loss : 0.5444\n",
      "epoch : 37, train loss : 1.1902, valid loss : 0.5463\n",
      "epoch : 38, train loss : 1.2140, valid loss : 0.5645\n",
      "epoch : 39, train loss : 1.2041, valid loss : 0.5805\n",
      "epoch : 40, train loss : 1.1715, valid loss : 0.5569\n",
      "epoch : 41, train loss : 1.1713, valid loss : 0.5561\n",
      "epoch : 42, train loss : 1.2058, valid loss : 0.5879\n",
      "epoch : 43, train loss : 1.2109, valid loss : 0.5977\n",
      "epoch : 44, train loss : 1.1795, valid loss : 0.5738\n",
      "epoch : 45, train loss : 1.1865, valid loss : 0.5317\n",
      "epoch : 46, train loss : 1.1633, valid loss : 0.5567\n",
      "epoch : 47, train loss : 1.2134, valid loss : 0.5491\n",
      "epoch : 48, train loss : 1.2311, valid loss : 0.5185\n",
      "epoch : 49, train loss : 1.2292, valid loss : 0.5249\n",
      "epoch : 50, train loss : 1.1916, valid loss : 0.5659\n",
      "epoch : 51, train loss : 1.2006, valid loss : 0.5295\n",
      "epoch : 52, train loss : 1.1380, valid loss : 0.5806\n",
      "epoch : 53, train loss : 1.1991, valid loss : 0.5453\n",
      "epoch : 54, train loss : 1.2458, valid loss : 0.5440\n",
      "epoch : 55, train loss : 1.1674, valid loss : 0.5653\n",
      "epoch : 56, train loss : 1.1806, valid loss : 0.5666\n",
      "epoch : 57, train loss : 1.1988, valid loss : 0.5521\n",
      "epoch : 58, train loss : 1.2399, valid loss : 0.5697\n",
      "epoch : 59, train loss : 1.1910, valid loss : 0.5888\n",
      "epoch : 60, train loss : 1.1907, valid loss : 0.5591\n",
      "epoch : 61, train loss : 1.2310, valid loss : 0.5485\n",
      "epoch : 62, train loss : 1.2475, valid loss : 0.5953\n",
      "epoch : 63, train loss : 1.1831, valid loss : 0.5725\n",
      "epoch : 64, train loss : 1.1955, valid loss : 0.5835\n",
      "epoch : 65, train loss : 1.1791, valid loss : 0.5410\n",
      "epoch : 66, train loss : 1.1654, valid loss : 0.5567\n",
      "epoch : 67, train loss : 1.1665, valid loss : 0.5677\n",
      "epoch : 68, train loss : 1.1884, valid loss : 0.5249\n",
      "epoch : 69, train loss : 1.1726, valid loss : 0.5591\n",
      "epoch : 70, train loss : 1.2069, valid loss : 0.5661\n",
      "epoch : 71, train loss : 1.2181, valid loss : 0.5565\n",
      "epoch : 72, train loss : 1.2379, valid loss : 0.5670\n",
      "epoch : 73, train loss : 1.2125, valid loss : 0.5215\n",
      "epoch : 74, train loss : 1.2146, valid loss : 0.5714\n",
      "epoch : 75, train loss : 1.1944, valid loss : 0.5416\n",
      "epoch : 76, train loss : 1.1930, valid loss : 0.6050\n",
      "epoch : 77, train loss : 1.1909, valid loss : 0.5537\n",
      "epoch : 78, train loss : 1.1846, valid loss : 0.5496\n",
      "epoch : 79, train loss : 1.1690, valid loss : 0.5837\n",
      "epoch : 80, train loss : 1.2169, valid loss : 0.5603\n",
      "epoch : 81, train loss : 1.2118, valid loss : 0.5304\n",
      "epoch : 82, train loss : 1.1700, valid loss : 0.5576\n",
      "epoch : 83, train loss : 1.1760, valid loss : 0.5576\n",
      "epoch : 84, train loss : 1.1846, valid loss : 0.6020\n",
      "epoch : 85, train loss : 1.1804, valid loss : 0.5581\n",
      "epoch : 86, train loss : 1.1978, valid loss : 0.5603\n",
      "epoch : 87, train loss : 1.1762, valid loss : 0.5499\n",
      "epoch : 88, train loss : 1.2270, valid loss : 0.5622\n",
      "epoch : 89, train loss : 1.2213, valid loss : 0.5719\n",
      "epoch : 90, train loss : 1.2033, valid loss : 0.5700\n",
      "epoch : 91, train loss : 1.2188, valid loss : 0.5951\n",
      "epoch : 92, train loss : 1.1802, valid loss : 0.5107\n",
      "epoch : 93, train loss : 1.2502, valid loss : 0.5228\n",
      "epoch : 94, train loss : 1.1617, valid loss : 0.5560\n",
      "epoch : 95, train loss : 1.1743, valid loss : 0.6002\n",
      "epoch : 96, train loss : 1.1972, valid loss : 0.5570\n",
      "epoch : 97, train loss : 1.2494, valid loss : 0.5561\n",
      "epoch : 98, train loss : 1.1721, valid loss : 0.5391\n",
      "epoch : 99, train loss : 1.1640, valid loss : 0.5650\n",
      "epoch : 100, train loss : 1.2095, valid loss : 0.5769\n",
      "epoch : 101, train loss : 1.1640, valid loss : 0.5813\n",
      "epoch : 102, train loss : 1.1696, valid loss : 0.5456\n",
      "epoch : 103, train loss : 1.1825, valid loss : 0.5545\n",
      "epoch : 104, train loss : 1.2289, valid loss : 0.5777\n",
      "epoch : 105, train loss : 1.1764, valid loss : 0.5626\n",
      "epoch : 106, train loss : 1.1671, valid loss : 0.5443\n",
      "epoch : 107, train loss : 1.2108, valid loss : 0.5739\n",
      "epoch : 108, train loss : 1.2002, valid loss : 0.5597\n",
      "epoch : 109, train loss : 1.1551, valid loss : 0.5637\n",
      "epoch : 110, train loss : 1.2150, valid loss : 0.5490\n",
      "epoch : 111, train loss : 1.1790, valid loss : 0.5482\n",
      "epoch : 112, train loss : 1.1780, valid loss : 0.5579\n",
      "epoch : 113, train loss : 1.1894, valid loss : 0.5389\n",
      "epoch : 114, train loss : 1.1655, valid loss : 0.5446\n",
      "epoch : 115, train loss : 1.2546, valid loss : 0.5491\n",
      "epoch : 116, train loss : 1.2014, valid loss : 0.5202\n",
      "epoch : 117, train loss : 1.1987, valid loss : 0.5555\n",
      "epoch : 118, train loss : 1.1814, valid loss : 0.5642\n",
      "epoch : 119, train loss : 1.2225, valid loss : 0.5393\n",
      "epoch : 120, train loss : 1.2342, valid loss : 0.5510\n",
      "epoch : 121, train loss : 1.1620, valid loss : 0.5505\n",
      "epoch : 122, train loss : 1.1991, valid loss : 0.5487\n",
      "epoch : 123, train loss : 1.2037, valid loss : 0.5515\n",
      "epoch : 124, train loss : 1.1836, valid loss : 0.6004\n",
      "epoch : 125, train loss : 1.2065, valid loss : 0.5871\n",
      "epoch : 126, train loss : 1.2346, valid loss : 0.5869\n",
      "epoch : 127, train loss : 1.1438, valid loss : 0.6045\n",
      "epoch : 128, train loss : 1.1797, valid loss : 0.5319\n",
      "epoch : 129, train loss : 1.1656, valid loss : 0.6122\n",
      "epoch : 130, train loss : 1.1959, valid loss : 0.5466\n",
      "epoch : 131, train loss : 1.2157, valid loss : 0.5667\n",
      "epoch : 132, train loss : 1.2118, valid loss : 0.6134\n",
      "epoch : 133, train loss : 1.1873, valid loss : 0.5500\n",
      "epoch : 134, train loss : 1.2047, valid loss : 0.5689\n",
      "epoch : 135, train loss : 1.2530, valid loss : 0.5453\n",
      "epoch : 136, train loss : 1.2006, valid loss : 0.5431\n",
      "epoch : 137, train loss : 1.2090, valid loss : 0.5502\n",
      "epoch : 138, train loss : 1.1666, valid loss : 0.5513\n",
      "epoch : 139, train loss : 1.2736, valid loss : 0.6016\n",
      "epoch : 140, train loss : 1.1966, valid loss : 0.5666\n",
      "epoch : 141, train loss : 1.1751, valid loss : 0.5690\n",
      "epoch : 142, train loss : 1.1551, valid loss : 0.5644\n",
      "epoch : 143, train loss : 1.1952, valid loss : 0.4913\n",
      "epoch : 144, train loss : 1.2229, valid loss : 0.5871\n",
      "epoch : 145, train loss : 1.2229, valid loss : 0.5423\n",
      "epoch : 146, train loss : 1.2420, valid loss : 0.5693\n",
      "epoch : 147, train loss : 1.1848, valid loss : 0.5615\n",
      "epoch : 148, train loss : 1.1986, valid loss : 0.5849\n",
      "epoch : 149, train loss : 1.1728, valid loss : 0.5700\n",
      "epoch : 150, train loss : 1.1906, valid loss : 0.5858\n",
      "epoch : 151, train loss : 1.1891, valid loss : 0.5345\n",
      "epoch : 152, train loss : 1.1857, valid loss : 0.5445\n",
      "epoch : 153, train loss : 1.2315, valid loss : 0.5717\n",
      "epoch : 154, train loss : 1.2056, valid loss : 0.5832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 155, train loss : 1.2304, valid loss : 0.5719\n",
      "epoch : 156, train loss : 1.2172, valid loss : 0.5792\n",
      "epoch : 157, train loss : 1.2017, valid loss : 0.5546\n",
      "epoch : 158, train loss : 1.1738, valid loss : 0.5411\n",
      "epoch : 159, train loss : 1.1751, valid loss : 0.5796\n",
      "epoch : 160, train loss : 1.1709, valid loss : 0.5843\n",
      "epoch : 161, train loss : 1.1988, valid loss : 0.5305\n",
      "epoch : 162, train loss : 1.1891, valid loss : 0.5724\n",
      "epoch : 163, train loss : 1.2277, valid loss : 0.5770\n",
      "epoch : 164, train loss : 1.1821, valid loss : 0.6067\n",
      "epoch : 165, train loss : 1.1844, valid loss : 0.5076\n",
      "epoch : 166, train loss : 1.1813, valid loss : 0.5485\n",
      "epoch : 167, train loss : 1.2011, valid loss : 0.5347\n",
      "epoch : 168, train loss : 1.1817, valid loss : 0.5595\n",
      "epoch : 169, train loss : 1.1936, valid loss : 0.5571\n",
      "epoch : 170, train loss : 1.1906, valid loss : 0.5697\n",
      "epoch : 171, train loss : 1.2107, valid loss : 0.5911\n",
      "epoch : 172, train loss : 1.2117, valid loss : 0.5374\n",
      "epoch : 173, train loss : 1.1974, valid loss : 0.5257\n",
      "epoch : 174, train loss : 1.1976, valid loss : 0.5533\n",
      "epoch : 175, train loss : 1.1953, valid loss : 0.5884\n",
      "epoch : 176, train loss : 1.2243, valid loss : 0.5773\n",
      "epoch : 177, train loss : 1.1820, valid loss : 0.5494\n",
      "epoch : 178, train loss : 1.1806, valid loss : 0.5515\n",
      "epoch : 179, train loss : 1.1591, valid loss : 0.6037\n",
      "epoch : 180, train loss : 1.1740, valid loss : 0.5724\n",
      "epoch : 181, train loss : 1.2562, valid loss : 0.5502\n",
      "epoch : 182, train loss : 1.1886, valid loss : 0.5693\n",
      "epoch : 183, train loss : 1.1615, valid loss : 0.5629\n",
      "epoch : 184, train loss : 1.1691, valid loss : 0.5715\n",
      "epoch : 185, train loss : 1.2172, valid loss : 0.5636\n",
      "epoch : 186, train loss : 1.2237, valid loss : 0.5518\n",
      "epoch : 187, train loss : 1.2003, valid loss : 0.6041\n",
      "epoch : 188, train loss : 1.2254, valid loss : 0.5606\n",
      "epoch : 189, train loss : 1.2013, valid loss : 0.5471\n",
      "epoch : 190, train loss : 1.2219, valid loss : 0.6310\n",
      "epoch : 191, train loss : 1.2203, valid loss : 0.5492\n",
      "epoch : 192, train loss : 1.2255, valid loss : 0.5949\n",
      "epoch : 193, train loss : 1.1853, valid loss : 0.5385\n",
      "epoch : 194, train loss : 1.1851, valid loss : 0.5699\n",
      "epoch : 195, train loss : 1.2015, valid loss : 0.5899\n",
      "epoch : 196, train loss : 1.2053, valid loss : 0.5251\n",
      "epoch : 197, train loss : 1.1705, valid loss : 0.5306\n",
      "epoch : 198, train loss : 1.2109, valid loss : 0.5762\n",
      "epoch : 199, train loss : 1.1980, valid loss : 0.5272\n",
      "epoch : 200, train loss : 1.2080, valid loss : 0.5762\n"
     ]
    }
   ],
   "source": [
    "mean_train_losses = []\n",
    "mean_valid_losses = []\n",
    "valid_acc_list = []\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for i, (embeddings, labels) in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(embeddings)\n",
    "        \n",
    "        #print(\"Embeddings: \", embeddings, \" Output: \", outputs, \" Labels: \", labels.view(-1,1))\n",
    "        \n",
    "        loss = torch.sqrt(loss_fn(outputs, labels.view(-1,1)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "            \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (embeddings, labels) in enumerate(valid_loader):\n",
    "            outputs = model(embeddings)\n",
    "            loss = torch.sqrt(loss_fn(outputs, labels.view(-1,1)))\n",
    "            \n",
    "            valid_losses.append(loss.item())\n",
    "            \n",
    "    mean_train_losses.append(np.mean(train_losses))\n",
    "    mean_valid_losses.append(np.mean(valid_losses))\n",
    "    \n",
    "    print('epoch : {}, train loss : {:.4f}, valid loss : {:.4f}'\\\n",
    "         .format(epoch+1, np.mean(train_losses), np.mean(valid_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T04:32:41.568398Z",
     "start_time": "2021-12-12T04:32:41.562240Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/multichannel_graph2vec_MLP_with_dropout_543_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T04:32:41.864596Z",
     "start_time": "2021-12-12T04:32:41.572341Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(mean_train_losses, label='train')\n",
    "ax.plot(mean_valid_losses, label='valid')\n",
    "lines, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(lines, labels, loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T04:34:07.132523Z",
     "start_time": "2021-12-12T04:34:06.272934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average test loss: 2.206455264772688\n",
      "average valid loss: 0.5609226760481085\n",
      "average train loss: 1.197193994508037\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_losses = []\n",
    "for i, (embeddings, labels) in enumerate(test_loader):\n",
    "    \n",
    "    pred = model(embeddings)\n",
    "    loss = torch.sqrt(loss_fn(pred, labels.view(-1,1)))\n",
    "    #print(\"Prediction:\", pred.detach().numpy(), \" Ground Truth:\", labels.view(-1,1))\n",
    "    test_losses.append(loss.item())\n",
    "print(\"average test loss:\", np.mean(test_losses))\n",
    "print(\"average valid loss:\", np.mean(mean_valid_losses))\n",
    "print(\"average train loss:\", np.mean(mean_train_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average r2_score: -1.6409089133775399\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score_average = []\n",
    "for i, (embeddings, labels) in enumerate(test_loader):\n",
    "    \n",
    "    pred = model(embeddings)\n",
    "    \n",
    "    r2_score_average.append(r2_score(labels.view(-1,1).detach().numpy(),pred.detach().numpy()))\n",
    "print(\"average r2_score:\", np.mean(r2_score_average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.1131735504783182"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(labels.view(-1,1).detach().numpy(),pred.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dropout - 0.5 0.4 0.3\n",
    "average test loss: 0.8273464867046901\n",
    "??\n",
    "??\n",
    "\n",
    "### dropout - 0.5 0.4 0.2\n",
    "average test loss: 0.8660806587764195\n",
    "average valid loss: 0.6826670368654387\n",
    "average train loss: 1.1280021055520706"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
